{
  "url": "https://www.tensorflow.org/guide/intro_to_graphs",
  "domain": "www.tensorflow.org",
  "category": "Technical Documentation",
  "timestamp": "2025-12-29T04:20:07.360692",
  "content": "TensorFlow\n\nLearn\n\nTensorFlow Core\n\nGuide\n\nIntroduction to graphs and tf.function\n\nStay organized with collections\n\nSave and categorize content based on your preferences.\n\nView on TensorFlow.org\n\nRun in Google Colab\n\nView source on GitHub\n\nDownload notebook\n\nOverview\n\nThis guide goes beneath the surface of TensorFlow and Keras to demonstrate how TensorFlow works. If you instead want to immediately get started with Keras, check out the\n\ncollection of Keras guides\n\n.\n\nIn this guide, you'll learn how TensorFlow allows you to make simple changes to your code to get graphs, how graphs are stored and represented, and how you can use them to accelerate your models.\n\nNote:\n\nFor those of you who are only familiar with TensorFlow 1.x, this guide demonstrates a very different view of graphs.\n\nThis is a big-picture overview that covers how\n\ntf.function\n\nallows you to switch from eager execution to graph execution.\n\nFor a more complete specification of\n\ntf.function\n\n, go to the\n\nBetter performance with\n\ntf.function\n\nguide.\n\nWhat are graphs?\n\nIn the previous three guides, you ran TensorFlow\n\neagerly\n\n. This means TensorFlow operations are executed by Python, operation by operation, and return results back to Python.\n\nWhile eager execution has several unique advantages, graph execution enables portability outside Python and tends to offer better performance.\n\nGraph execution\n\nmeans that tensor computations are executed as a\n\nTensorFlow graph\n\n, sometimes referred to as a\n\ntf.Graph\n\nor simply a \"graph.\"\n\nGraphs are data structures that contain a set of\n\ntf.Operation\n\nobjects, which represent units of computation; and\n\ntf.Tensor\n\nobjects, which represent the units of data that flow between operations.\n\nThey are defined in a\n\ntf.Graph\n\ncontext. Since these graphs are data structures, they can be saved, run, and restored all without the original Python code.\n\nThis is what a TensorFlow graph representing a two-layer neural network looks like when visualized in TensorBoard:\n\nThe benefits of graphs\n\nWith a graph, you have a great deal of flexibility.  You can use your TensorFlow graph in environments that don't have a Python interpreter, like mobile applications, embedded devices, and backend servers. TensorFlow uses graphs as the format for\n\nsaved models\n\nwhen it exports them from Python.\n\nGraphs are also easily optimized, allowing the compiler to do transformations like:\n\nStatically infer the value of tensors by folding constant nodes in your computation\n\n(\"constant folding\")\n\n.\n\nSeparate sub-parts of a computation that are independent and split them between threads or devices.\n\nSimplify arithmetic operations by eliminating common subexpressions.\n\nThere is an entire optimization system,\n\nGrappler\n\n, to perform this and other speedups.\n\nIn short, graphs are extremely useful and let your TensorFlow run\n\nfast\n\n, run\n\nin parallel\n\n, and run efficiently\n\non multiple devices\n\n.\n\nHowever, you still want to define your machine learning models (or other computations) in Python for convenience, and then automatically construct graphs when you need them.\n\nSetup\n\nImport some necessary libraries:\n\nimport\n\ntensorflow\n\nas\n\ntf\n\nimport\n\ntimeit\n\nfrom\n\ndatetime\n\nimport\n\ndatetime\n\n2024-08-15 01:23:58.511668: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-15 01:23:58.532403: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-15 01:23:58.538519: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\nTaking advantage of graphs\n\nYou create and run a graph in TensorFlow by using\n\ntf.function\n\n, either as a direct call or as a decorator.\n\ntf.function\n\ntakes a regular function as input and returns a\n\ntf.types.experimental.PolymorphicFunction\n\n.\n\nA\n\nPolymorphicFunction\n\nis a Python callable that builds TensorFlow graphs from the Python function. You use a\n\ntf.function\n\nin the same way as its Python equivalent.\n\n# Define a Python function.\n\ndef\n\na_regular_function\n\n(\n\nx\n\n,\n\ny\n\n,\n\nb\n\n):\n\nx\n\n=\n\ntf\n\n.\n\nmatmul\n\n(\n\nx\n\n,\n\ny\n\n)\n\nx\n\n=\n\nx\n\n+\n\nb\n\nreturn\n\nx\n\n# The Python type of `a_function_that_uses_a_graph` will now be a\n\n# `PolymorphicFunction`.\n\na_function_that_uses_a_graph\n\n=\n\ntf\n\n.\n\nfunction\n\n(\n\na_regular_function\n\n)\n\n# Make some tensors.\n\nx1\n\n=\n\ntf\n\n.\n\nconstant\n\n([[\n\n1.0\n\n,\n\n2.0\n\n]])\n\ny1\n\n=\n\ntf\n\n.\n\nconstant\n\n([[\n\n2.0\n\n],\n\n[\n\n3.0\n\n]])\n\nb1\n\n=\n\ntf\n\n.\n\nconstant\n\n(\n\n4.0\n\n)\n\norig_value\n\n=\n\na_regular_function\n\n(\n\nx1\n\n,\n\ny1\n\n,\n\nb1\n\n)\n\n.\n\nnumpy\n\n()\n\n# Call a `tf.function` like a Python function.\n\ntf_function_value\n\n=\n\na_function_that_uses_a_graph\n\n(\n\nx1\n\n,\n\ny1\n\n,\n\nb1\n\n)\n\n.\n\nnumpy\n\n()\n\nassert\n\n(\n\norig_value\n\n==\n\ntf_function_value\n\n)\n\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1723685041.078349   10585 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685041.081709   10585 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685041.084876   10585 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685041.088691   10585 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685041.100124   10585 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685041.103158   10585 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685041.106072   10585 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685041.109491   10585 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685041.112991   10585 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685041.115870   10585 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685041.118785   10585 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685041.122189   10585 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685042.369900   10585 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685042.372045   10585 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685042.374040   10585 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685042.376123   10585 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685042.378174   10585 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685042.380184   10585 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685042.382098   10585 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685042.384064   10585 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685042.386002   10585 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685042.387981   10585 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685042.389902   10585 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685042.391922   10585 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685042.431010   10585 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685042.433093   10585 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685042.435050   10585 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685042.437074   10585 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685042.439053   10585 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685042.441049   10585 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685042.442965   10585 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685042.444941   10585 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685042.446890   10585 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685042.450623   10585 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685042.453482   10585 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685042.455908   10585 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n\nOn the outside, a\n\ntf.function\n\nlooks like a regular function you write using TensorFlow operations.\n\nUnderneath\n\n, however, it is\n\nvery different\n\n. The underlying\n\nPolymorphicFunction\n\nencapsulates several\n\ntf.Graph\n\ns behind one API\n\n(learn more in the\n\nPolymorphism\n\nsection). That is how a\n\ntf.function\n\nis able to give you the benefits of graph execution, like speed and deployability (refer to\n\nThe benefits of graphs\n\nabove).\n\ntf.function\n\napplies to a function\n\nand all other functions it calls\n\n:\n\ndef\n\ninner_function\n\n(\n\nx\n\n,\n\ny\n\n,\n\nb\n\n):\n\nx\n\n=\n\ntf\n\n.\n\nmatmul\n\n(\n\nx\n\n,\n\ny\n\n)\n\nx\n\n=\n\nx\n\n+\n\nb\n\nreturn\n\nx\n\n# Using the `tf.function` decorator makes `outer_function` into a\n\n# `PolymorphicFunction`.\n\n@tf\n\n.\n\nfunction\n\ndef\n\nouter_function\n\n(\n\nx\n\n):\n\ny\n\n=\n\ntf\n\n.\n\nconstant\n\n([[\n\n2.0\n\n],\n\n[\n\n3.0\n\n]])\n\nb\n\n=\n\ntf\n\n.\n\nconstant\n\n(\n\n4.0\n\n)\n\nreturn\n\ninner_function\n\n(\n\nx\n\n,\n\ny\n\n,\n\nb\n\n)\n\n# Note that the callable will create a graph that\n\n# includes `inner_function` as well as `outer_function`.\n\nouter_function\n\n(\n\ntf\n\n.\n\nconstant\n\n([[\n\n1.0\n\n,\n\n2.0\n\n]]))\n\n.\n\nnumpy\n\n()\n\narray([[12.]], dtype=float32)\n\nIf you have used TensorFlow 1.x, you will notice that at no time did you need to define a\n\nPlaceholder\n\nor\n\ntf.Session\n\n.\n\nConverting Python functions to graphs\n\nAny function you write with TensorFlow will contain a mixture of built-in TF operations and Python logic, such as\n\nif-then\n\nclauses, loops,\n\nbreak\n\n,\n\nreturn\n\n,\n\ncontinue\n\n, and more. While TensorFlow operations are easily captured by a\n\ntf.Graph\n\n, Python-specific logic needs to undergo an extra step in order to become part of the graph.\n\ntf.function\n\nuses a library called AutoGraph (\n\ntf.autograph\n\n) to convert Python code into graph-generating code.\n\ndef\n\nsimple_relu\n\n(\n\nx\n\n):\n\nif\n\ntf\n\n.\n\ngreater\n\n(\n\nx\n\n,\n\n0\n\n):\n\nreturn\n\nx\n\nelse\n\n:\n\nreturn\n\n0\n\n# Using `tf.function` makes `tf_simple_relu` a `PolymorphicFunction` that wraps\n\n# `simple_relu`.\n\ntf_simple_relu\n\n=\n\ntf\n\n.\n\nfunction\n\n(\n\nsimple_relu\n\n)\n\nprint\n\n(\n\n\"First branch, with graph:\"\n\n,\n\ntf_simple_relu\n\n(\n\ntf\n\n.\n\nconstant\n\n(\n\n1\n\n))\n\n.\n\nnumpy\n\n())\n\nprint\n\n(\n\n\"Second branch, with graph:\"\n\n,\n\ntf_simple_relu\n\n(\n\ntf\n\n.\n\nconstant\n\n(\n\n-\n\n1\n\n))\n\n.\n\nnumpy\n\n())\n\nFirst branch, with graph: 1\nSecond branch, with graph: 0\n\nThough it is unlikely that you will need to view graphs directly, you can inspect the outputs to check the exact results. These are not easy to read, so no need to look too carefully!\n\n# This is the graph-generating output of AutoGraph.\n\nprint\n\n(\n\ntf\n\n.\n\nautograph\n\n.\n\nto_code\n\n(\n\nsimple_relu\n\n))\n\ndef tf__simple_relu(x):\n    with ag__.FunctionScope('simple_relu', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as fscope:\n        do_return = False\n        retval_ = ag__.UndefinedReturnValue()\n\n        def get_state():\n            return (do_return, retval_)\n\n        def set_state(vars_):\n            nonlocal do_return, retval_\n            (do_return, retval_) = vars_\n\n        def if_body():\n            nonlocal do_return, retval_\n            try:\n                do_return = True\n                retval_ = ag__.ld(x)\n            except:\n                do_return = False\n                raise\n\n        def else_body():\n            nonlocal do_return, retval_\n            try:\n                do_return = True\n                retval_ = 0\n            except:\n                do_return = False\n                raise\n        ag__.if_stmt(ag__.converted_call(ag__.ld(tf).greater, (ag__.ld(x), 0), None, fscope), if_body, else_body, get_state, set_state, ('do_return', 'retval_'), 2)\n        return fscope.ret(retval_, do_return)\n\n# This is the graph itself.\n\nprint\n\n(\n\ntf_simple_relu\n\n.\n\nget_concrete_function\n\n(\n\ntf\n\n.\n\nconstant\n\n(\n\n1\n\n))\n\n.\n\ngraph\n\n.\n\nas_graph_def\n\n())\n\nnode {\n  name: \"x\"\n  op: \"Placeholder\"\n  attr {\n    key: \"_user_specified_name\"\n    value {\n      s: \"x\"\n    }\n  }\n  attr {\n    key: \"dtype\"\n    value {\n      type: DT_INT32\n    }\n  }\n  attr {\n    key: \"shape\"\n    value {\n      shape {\n      }\n    }\n  }\n}\nnode {\n  name: \"Greater/y\"\n  op: \"Const\"\n  attr {\n    key: \"dtype\"\n    value {\n      type: DT_INT32\n    }\n  }\n  attr {\n    key: \"value\"\n    value {\n      tensor {\n        dtype: DT_INT32\n        tensor_shape {\n        }\n        int_val: 0\n      }\n    }\n  }\n}\nnode {\n  name: \"Greater\"\n  op: \"Greater\"\n  input: \"x\"\n  input: \"Greater/y\"\n  attr {\n    key: \"T\"\n    value {\n      type: DT_INT32\n    }\n  }\n}\nnode {\n  name: \"cond\"\n  op: \"StatelessIf\"\n  input: \"Greater\"\n  input: \"x\"\n  attr {\n    key: \"Tcond\"\n    value {\n      type: DT_BOOL\n    }\n  }\n  attr {\n    key: \"Tin\"\n    value {\n      list {\n        type: DT_INT32\n      }\n    }\n  }\n  attr {\n    key: \"Tout\"\n    value {\n      list {\n        type: DT_BOOL\n        type: DT_INT32\n      }\n    }\n  }\n  attr {\n    key: \"_lower_using_switch_merge\"\n    value {\n      b: true\n    }\n  }\n  attr {\n    key: \"_read_only_resource_inputs\"\n    value {\n      list {\n      }\n    }\n  }\n  attr {\n    key: \"else_branch\"\n    value {\n      func {\n        name: \"cond_false_31\"\n      }\n    }\n  }\n  attr {\n    key: \"output_shapes\"\n    value {\n      list {\n        shape {\n        }\n        shape {\n        }\n      }\n    }\n  }\n  attr {\n    key: \"then_branch\"\n    value {\n      func {\n        name: \"cond_true_30\"\n      }\n    }\n  }\n}\nnode {\n  name: \"cond/Identity\"\n  op: \"Identity\"\n  input: \"cond\"\n  attr {\n    key: \"T\"\n    value {\n      type: DT_BOOL\n    }\n  }\n}\nnode {\n  name: \"cond/Identity_1\"\n  op: \"Identity\"\n  input: \"cond:1\"\n  attr {\n    key: \"T\"\n    value {\n      type: DT_INT32\n    }\n  }\n}\nnode {\n  name: \"Identity\"\n  op: \"Identity\"\n  input: \"cond/Identity_1\"\n  attr {\n    key: \"T\"\n    value {\n      type: DT_INT32\n    }\n  }\n}\nlibrary {\n  function {\n    signature {\n      name: \"cond_false_31\"\n      input_arg {\n        name: \"cond_placeholder\"\n        type: DT_INT32\n      }\n      output_arg {\n        name: \"cond_identity\"\n        type: DT_BOOL\n      }\n      output_arg {\n        name: \"cond_identity_1\"\n        type: DT_INT32\n      }\n    }\n    node_def {\n      name: \"cond/Const\"\n      op: \"Const\"\n      attr {\n        key: \"dtype\"\n        value {\n          type: DT_BOOL\n        }\n      }\n      attr {\n        key: \"value\"\n        value {\n          tensor {\n            dtype: DT_BOOL\n            tensor_shape {\n            }\n            bool_val: true\n          }\n        }\n      }\n    }\n    node_def {\n      name: \"cond/Const_1\"\n      op: \"Const\"\n      attr {\n        key: \"dtype\"\n        value {\n          type: DT_BOOL\n        }\n      }\n      attr {\n        key: \"value\"\n        value {\n          tensor {\n            dtype: DT_BOOL\n            tensor_shape {\n            }\n            bool_val: true\n          }\n        }\n      }\n    }\n    node_def {\n      name: \"cond/Const_2\"\n      op: \"Const\"\n      attr {\n        key: \"dtype\"\n        value {\n          type: DT_INT32\n        }\n      }\n      attr {\n        key: \"value\"\n        value {\n          tensor {\n            dtype: DT_INT32\n            tensor_shape {\n            }\n            int_val: 0\n          }\n        }\n      }\n    }\n    node_def {\n      name: \"cond/Const_3\"\n      op: \"Const\"\n      attr {\n        key: \"dtype\"\n        value {\n          type: DT_BOOL\n        }\n      }\n      attr {\n        key: \"value\"\n        value {\n          tensor {\n            dtype: DT_BOOL\n            tensor_shape {\n            }\n            bool_val: true\n          }\n        }\n      }\n    }\n    node_def {\n      name: \"cond/Identity\"\n      op: \"Identity\"\n      input: \"cond/Const_3:output:0\"\n      attr {\n        key: \"T\"\n        value {\n          type: DT_BOOL\n        }\n      }\n    }\n    node_def {\n      name: \"cond/Const_4\"\n      op: \"Const\"\n      attr {\n        key: \"dtype\"\n        value {\n          type: DT_INT32\n        }\n      }\n      attr {\n        key: \"value\"\n        value {\n          tensor {\n            dtype: DT_INT32\n            tensor_shape {\n            }\n            int_val: 0\n          }\n        }\n      }\n    }\n    node_def {\n      name: \"cond/Identity_1\"\n      op: \"Identity\"\n      input: \"cond/Const_4:output:0\"\n      attr {\n        key: \"T\"\n        value {\n          type: DT_INT32\n        }\n      }\n    }\n    ret {\n      key: \"cond_identity\"\n      value: \"cond/Identity:output:0\"\n    }\n    ret {\n      key: \"cond_identity_1\"\n      value: \"cond/Identity_1:output:0\"\n    }\n    attr {\n      key: \"_construction_context\"\n      value {\n        s: \"kEagerRuntime\"\n      }\n    }\n    arg_attr {\n      key: 0\n      value {\n        attr {\n          key: \"_output_shapes\"\n          value {\n            list {\n              shape {\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n  function {\n    signature {\n      name: \"cond_true_30\"\n      input_arg {\n        name: \"cond_identity_1_x\"\n        type: DT_INT32\n      }\n      output_arg {\n        name: \"cond_identity\"\n        type: DT_BOOL\n      }\n      output_arg {\n        name: \"cond_identity_1\"\n        type: DT_INT32\n      }\n    }\n    node_def {\n      name: \"cond/Const\"\n      op: \"Const\"\n      attr {\n        key: \"dtype\"\n        value {\n          type: DT_BOOL\n        }\n      }\n      attr {\n        key: \"value\"\n        value {\n          tensor {\n            dtype: DT_BOOL\n            tensor_shape {\n            }\n            bool_val: true\n          }\n        }\n      }\n    }\n    node_def {\n      name: \"cond/Identity\"\n      op: \"Identity\"\n      input: \"cond/Const:output:0\"\n      attr {\n        key: \"T\"\n        value {\n          type: DT_BOOL\n        }\n      }\n    }\n    node_def {\n      name: \"cond/Identity_1\"\n      op: \"Identity\"\n      input: \"cond_identity_1_x\"\n      attr {\n        key: \"T\"\n        value {\n          type: DT_INT32\n        }\n      }\n    }\n    ret {\n      key: \"cond_identity\"\n      value: \"cond/Identity:output:0\"\n    }\n    ret {\n      key: \"cond_identity_1\"\n      value: \"cond/Identity_1:output:0\"\n    }\n    attr {\n      key: \"_construction_context\"\n      value {\n        s: \"kEagerRuntime\"\n      }\n    }\n    arg_attr {\n      key: 0\n      value {\n        attr {\n          key: \"_output_shapes\"\n          value {\n            list {\n              shape {\n              }\n            }\n          }\n        }\n        attr {\n          key: \"_user_specified_name\"\n          value {\n            s: \"x\"\n          }\n        }\n      }\n    }\n  }\n}\nversions {\n  producer: 1882\n  min_consumer: 12\n}\n\nMost of the time,\n\ntf.function\n\nwill work without  special considerations. However, there are some caveats, and the\n\ntf.function\n\nguide\n\ncan help here, as well as the\n\ncomplete AutoGraph reference\n\n.\n\nPolymorphism: one\n\ntf.function\n\n, many graphs\n\nA\n\ntf.Graph\n\nis specialized to a specific type of inputs (for example, tensors with a specific\n\ndtype\n\nor objects with the same\n\nid()\n\n).\n\nEach time you invoke a\n\ntf.function\n\nwith a set of arguments that can't be handled by any of its existing graphs (such as arguments with new\n\ndtypes\n\nor incompatible shapes), it creates a new\n\ntf.Graph\n\nspecialized to those new arguments. The type specification of a\n\ntf.Graph\n\n's inputs is represented by\n\ntf.types.experimental.FunctionType\n\n, also referred to as the\n\nsignature\n\n. For more information regarding when a new\n\ntf.Graph\n\nis generated, how that can be controlled, and how\n\nFunctionType\n\ncan be useful, go to the\n\nRules of tracing\n\nsection of the\n\nBetter performance with\n\ntf.function\n\nguide.\n\nThe\n\ntf.function\n\nstores the\n\ntf.Graph\n\ncorresponding to that signature in a\n\nConcreteFunction\n\n.\n\nA\n\nConcreteFunction\n\ncan be thought of as a wrapper around a\n\ntf.Graph\n\n.\n\n@tf\n\n.\n\nfunction\n\ndef\n\nmy_relu\n\n(\n\nx\n\n):\n\nreturn\n\ntf\n\n.\n\nmaximum\n\n(\n\n0.\n\n,\n\nx\n\n)\n\n# `my_relu` creates new graphs as it observes different input types.\n\nprint\n\n(\n\nmy_relu\n\n(\n\ntf\n\n.\n\nconstant\n\n(\n\n5.5\n\n)))\n\nprint\n\n(\n\nmy_relu\n\n([\n\n1\n\n,\n\n-\n\n1\n\n]))\n\nprint\n\n(\n\nmy_relu\n\n(\n\ntf\n\n.\n\nconstant\n\n([\n\n3.\n\n,\n\n-\n\n3.\n\n])))\n\ntf.Tensor(5.5, shape=(), dtype=float32)\ntf.Tensor([1. 0.], shape=(2,), dtype=float32)\ntf.Tensor([3. 0.], shape=(2,), dtype=float32)\n\nIf the\n\ntf.function\n\nhas already been called with the same input types, it does not create a new\n\ntf.Graph\n\n.\n\n# These two calls do *not* create new graphs.\n\nprint\n\n(\n\nmy_relu\n\n(\n\ntf\n\n.\n\nconstant\n\n(\n\n-\n\n2.5\n\n)))\n\n# Input type matches `tf.constant(5.5)`.\n\nprint\n\n(\n\nmy_relu\n\n(\n\ntf\n\n.\n\nconstant\n\n([\n\n-\n\n1.\n\n,\n\n1.\n\n])))\n\n# Input type matches `tf.constant([3., -3.])`.\n\ntf.Tensor(0.0, shape=(), dtype=float32)\ntf.Tensor([0. 1.], shape=(2,), dtype=float32)\n\nBecause it's backed by multiple graphs, a\n\ntf.function\n\nis (as the name \"PolymorphicFunction\" suggests)\n\npolymorphic\n\n. That enables it to support more input types than a single\n\ntf.Graph\n\ncould represent, and to optimize each\n\ntf.Graph\n\nfor better performance.\n\n# There are three `ConcreteFunction`s (one for each graph) in `my_relu`.\n\n# The `ConcreteFunction` also knows the return type and shape!\n\nprint\n\n(\n\nmy_relu\n\n.\n\npretty_printed_concrete_signatures\n\n())\n\nInput Parameters:\n  x (POSITIONAL_OR_KEYWORD): TensorSpec(shape=(), dtype=tf.float32, name=None)\nOutput Type:\n  TensorSpec(shape=(), dtype=tf.float32, name=None)\nCaptures:\n  None\n\nInput Parameters:\n  x (POSITIONAL_OR_KEYWORD): List[Literal[1], Literal[-1]]\nOutput Type:\n  TensorSpec(shape=(2,), dtype=tf.float32, name=None)\nCaptures:\n  None\n\nInput Parameters:\n  x (POSITIONAL_OR_KEYWORD): TensorSpec(shape=(2,), dtype=tf.float32, name=None)\nOutput Type:\n  TensorSpec(shape=(2,), dtype=tf.float32, name=None)\nCaptures:\n  None\n\nUsing\n\ntf.function\n\nSo far, you've learned how to convert a Python function into a graph simply by using\n\ntf.function\n\nas a decorator or wrapper. But in practice, getting\n\ntf.function\n\nto work correctly can be tricky! In the following sections, you'll learn how you can make your code work as expected with\n\ntf.function\n\n.\n\nGraph execution vs. eager execution\n\nThe code in a\n\ntf.function\n\ncan be executed both eagerly and as a graph. By default,\n\ntf.function\n\nexecutes its code as a graph:\n\n@tf\n\n.\n\nfunction\n\ndef\n\nget_MSE\n\n(\n\ny_true\n\n,\n\ny_pred\n\n):\n\nsq_diff\n\n=\n\ntf\n\n.\n\npow\n\n(\n\ny_true\n\n-\n\ny_pred\n\n,\n\n2\n\n)\n\nreturn\n\ntf\n\n.\n\nreduce_mean\n\n(\n\nsq_diff\n\n)\n\ny_true\n\n=\n\ntf\n\n.\n\nrandom\n\n.\n\nuniform\n\n([\n\n5\n\n],\n\nmaxval\n\n=\n\n10\n\n,\n\ndtype\n\n=\n\ntf\n\n.\n\nint32\n\n)\n\ny_pred\n\n=\n\ntf\n\n.\n\nrandom\n\n.\n\nuniform\n\n([\n\n5\n\n],\n\nmaxval\n\n=\n\n10\n\n,\n\ndtype\n\n=\n\ntf\n\n.\n\nint32\n\n)\n\nprint\n\n(\n\ny_true\n\n)\n\nprint\n\n(\n\ny_pred\n\n)\n\ntf.Tensor([2 0 7 2 3], shape=(5,), dtype=int32)\ntf.Tensor([9 9 1 1 5], shape=(5,), dtype=int32)\n\nget_MSE\n\n(\n\ny_true\n\n,\n\ny_pred\n\n)\n\n<tf.Tensor: shape=(), dtype=int32, numpy=34>\n\nTo verify that your\n\ntf.function\n\n's graph is doing the same computation as its equivalent Python function, you can make it execute eagerly with\n\ntf.config.run_functions_eagerly(True)\n\n. This is a switch that\n\nturns off\n\ntf.function\n\n's ability to create and run graphs\n\n, instead of executing the code normally.\n\ntf\n\n.\n\nconfig\n\n.\n\nrun_functions_eagerly\n\n(\n\nTrue\n\n)\n\nget_MSE\n\n(\n\ny_true\n\n,\n\ny_pred\n\n)\n\n<tf.Tensor: shape=(), dtype=int32, numpy=34>\n\n# Don't forget to set it back when you are done.\n\ntf\n\n.\n\nconfig\n\n.\n\nrun_functions_eagerly\n\n(\n\nFalse\n\n)\n\nHowever,\n\ntf.function\n\ncan behave differently under graph and eager execution. The Python\n\nprint\n\nfunction is one example of how these two modes differ. Let's check out what happens when you insert a\n\nprint\n\nstatement to your function and call it repeatedly.\n\n@tf\n\n.\n\nfunction\n\ndef\n\nget_MSE\n\n(\n\ny_true\n\n,\n\ny_pred\n\n):\n\nprint\n\n(\n\n\"Calculating MSE!\"\n\n)\n\nsq_diff\n\n=\n\ntf\n\n.\n\npow\n\n(\n\ny_true\n\n-\n\ny_pred\n\n,\n\n2\n\n)\n\nreturn\n\ntf\n\n.\n\nreduce_mean\n\n(\n\nsq_diff\n\n)\n\nObserve what is printed:\n\nerror\n\n=\n\nget_MSE\n\n(\n\ny_true\n\n,\n\ny_pred\n\n)\n\nerror\n\n=\n\nget_MSE\n\n(\n\ny_true\n\n,\n\ny_pred\n\n)\n\nerror\n\n=\n\nget_MSE\n\n(\n\ny_true\n\n,\n\ny_pred\n\n)\n\nCalculating MSE!\n\nIs the output surprising?\n\nget_MSE\n\nonly printed once even though it was called\n\nthree\n\ntimes.\n\nTo explain, the\n\nprint\n\nstatement is executed when\n\ntf.function\n\nruns the original code in order to create the graph in a process known as \"tracing\" (refer to the\n\nTracing\n\nsection of the\n\ntf.function\n\nguide\n\n.\n\nTracing captures the TensorFlow operations into a graph, and\n\nprint\n\nis not captured in the graph.\n\nThat graph is then executed for all three calls\n\nwithout ever running the Python code again\n\n.\n\nAs a sanity check, let's turn off graph execution to compare:\n\n# Now, globally set everything to run eagerly to force eager execution.\n\ntf\n\n.\n\nconfig\n\n.\n\nrun_functions_eagerly\n\n(\n\nTrue\n\n)\n\n# Observe what is printed below.\n\nerror\n\n=\n\nget_MSE\n\n(\n\ny_true\n\n,\n\ny_pred\n\n)\n\nerror\n\n=\n\nget_MSE\n\n(\n\ny_true\n\n,\n\ny_pred\n\n)\n\nerror\n\n=\n\nget_MSE\n\n(\n\ny_true\n\n,\n\ny_pred\n\n)\n\nCalculating MSE!\nCalculating MSE!\nCalculating MSE!\n\ntf\n\n.\n\nconfig\n\n.\n\nrun_functions_eagerly\n\n(\n\nFalse\n\n)\n\nprint\n\nis a\n\nPython side effect\n\n, and there are other differences that you should be aware of when converting a function into a\n\ntf.function\n\n. Learn more in the\n\nLimitations\n\nsection of the\n\nBetter performance with\n\ntf.function\n\nguide.\n\nNote:\n\nIf you would like to print values in both eager and graph execution, use\n\ntf.print\n\ninstead.\n\nNon-strict execution\n\nGraph execution only executes the operations necessary to produce the observable effects, which include:\n\nThe return value of the function\n\nDocumented well-known side-effects such as:\n\nInput/output operations, like\n\ntf.print\n\nDebugging operations, such as the assert functions in\n\ntf.debugging\n\nMutations of\n\ntf.Variable\n\nThis behavior is usually known as \"Non-strict execution\", and differs from eager execution, which steps through all of the program operations, needed or not.\n\nIn particular, runtime error checking does not count as an observable effect. If an operation is skipped because it is unnecessary, it cannot raise any runtime errors.\n\nIn the following example, the \"unnecessary\" operation\n\ntf.gather\n\nis skipped during graph execution, so the runtime error\n\nInvalidArgumentError\n\nis not raised as it would be in eager execution. Do not rely on an error being raised while executing a graph.\n\ndef\n\nunused_return_eager\n\n(\n\nx\n\n):\n\n# Get index 1 will fail when `len(x) == 1`\n\ntf\n\n.\n\ngather\n\n(\n\nx\n\n,\n\n[\n\n1\n\n])\n\n# unused\n\nreturn\n\nx\n\ntry\n\n:\n\nprint\n\n(\n\nunused_return_eager\n\n(\n\ntf\n\n.\n\nconstant\n\n([\n\n0.0\n\n])))\n\nexcept\n\ntf\n\n.\n\nerrors\n\n.\n\nInvalidArgumentError\n\nas\n\ne\n\n:\n\n# All operations are run during eager execution so an error is raised.\n\nprint\n\n(\n\nf\n\n'\n\n{\n\ntype\n\n(\n\ne\n\n)\n\n.\n\n__name__\n\n}\n\n:\n\n{\n\ne\n\n}\n\n'\n\n)\n\ntf.Tensor([0.], shape=(1,), dtype=float32)\n\n@tf\n\n.\n\nfunction\n\ndef\n\nunused_return_graph\n\n(\n\nx\n\n):\n\ntf\n\n.\n\ngather\n\n(\n\nx\n\n,\n\n[\n\n1\n\n])\n\n# unused\n\nreturn\n\nx\n\n# Only needed operations are run during graph execution. The error is not raised.\n\nprint\n\n(\n\nunused_return_graph\n\n(\n\ntf\n\n.\n\nconstant\n\n([\n\n0.0\n\n])))\n\ntf.Tensor([0.], shape=(1,), dtype=float32)\n\ntf.function\n\nbest practices\n\nIt may take some time to get used to the behavior of\n\ntf.function\n\n.  To get started quickly, first-time users should play around with decorating toy functions with\n\n@tf.function\n\nto get experience with going from eager to graph execution.\n\nDesigning for\n\ntf.function\n\nmay be your best bet for writing graph-compatible TensorFlow programs. Here are some tips:\n\nToggle between eager and graph execution early and often with\n\ntf.config.run_functions_eagerly\n\nto pinpoint if/ when the two modes diverge.\n\nCreate\n\ntf.Variable\n\ns\noutside the Python function and modify them on the inside. The same goes for objects that use\n\ntf.Variable\n\n, like\n\ntf.keras.layers\n\n,\n\ntf.keras.Model\n\ns and\n\ntf.keras.optimizers\n\n.\n\nAvoid writing functions that depend on outer Python variables, excluding\n\ntf.Variable\n\ns and Keras objects. Learn more in\n\nDepending on Python global and free variables\n\nof the\n\ntf.function\n\nguide\n\n.\n\nPrefer to write functions which take tensors and other TensorFlow types as input. You can pass in other object types but be careful! Learn more in\n\nDepending on Python objects\n\nof the\n\ntf.function\n\nguide\n\n.\n\nInclude as much computation as possible under a\n\ntf.function\n\nto maximize the performance gain. For example, decorate a whole training step or the entire training loop.\n\nSeeing the speed-up\n\ntf.function\n\nusually improves the performance of your code, but the amount of speed-up depends on the kind of computation you run. Small computations can be dominated by the overhead of calling a graph. You can measure the difference in performance like so:\n\nx\n\n=\n\ntf\n\n.\n\nrandom\n\n.\n\nuniform\n\n(\n\nshape\n\n=\n\n[\n\n10\n\n,\n\n10\n\n],\n\nminval\n\n=-\n\n1\n\n,\n\nmaxval\n\n=\n\n2\n\n,\n\ndtype\n\n=\n\ntf\n\n.\n\ndtypes\n\n.\n\nint32\n\n)\n\ndef\n\npower\n\n(\n\nx\n\n,\n\ny\n\n):\n\nresult\n\n=\n\ntf\n\n.\n\neye\n\n(\n\n10\n\n,\n\ndtype\n\n=\n\ntf\n\n.\n\ndtypes\n\n.\n\nint32\n\n)\n\nfor\n\n_\n\nin\n\nrange\n\n(\n\ny\n\n):\n\nresult\n\n=\n\ntf\n\n.\n\nmatmul\n\n(\n\nx\n\n,\n\nresult\n\n)\n\nreturn\n\nresult\n\nprint\n\n(\n\n\"Eager execution:\"\n\n,\n\ntimeit\n\n.\n\ntimeit\n\n(\n\nlambda\n\n:\n\npower\n\n(\n\nx\n\n,\n\n100\n\n),\n\nnumber\n\n=\n\n1000\n\n),\n\n\"seconds\"\n\n)\n\nEager execution: 4.1027931490000356 seconds\n\npower_as_graph\n\n=\n\ntf\n\n.\n\nfunction\n\n(\n\npower\n\n)\n\nprint\n\n(\n\n\"Graph execution:\"\n\n,\n\ntimeit\n\n.\n\ntimeit\n\n(\n\nlambda\n\n:\n\npower_as_graph\n\n(\n\nx\n\n,\n\n100\n\n),\n\nnumber\n\n=\n\n1000\n\n),\n\n\"seconds\"\n\n)\n\nGraph execution: 0.7951284349999241 seconds\n\ntf.function\n\nis commonly used to speed up training loops, and you can learn more about it in the _Speeding-up your training step with\n\ntf.function\n\n_ section of the\n\nWriting a training loop from scratch\n\nwith Keras guide.\n\nNote:\n\nYou can also try\n\ntf.function(jit_compile=True)\n\nfor a more significant performance boost, especially if your code is heavy on TensorFlow control flow and uses many small tensors. Learn more in the _Explicit compilation with\n\ntf.function(jit\n\ncompile=True)\n\nsection of the\n\nXLA overview\n\n.\n\nPerformance and trade-offs\n\nGraphs can speed up your code, but the process of creating them has some overhead. For some functions, the creation of the graph takes more time than the execution of the graph.\n\nThis investment is usually quickly paid back with the performance boost of subsequent executions, but it's important to be aware that the first few  steps of any large model training can be slower due to tracing.\n\nNo matter how large your model, you want to avoid tracing frequently. In the\n\nControlling retracing\n\nsection, the\n\ntf.function\n\nguide\n\ndiscusses how to set input specifications and use tensor arguments to avoid retracing. If you find you are getting unusually poor performance, it's a good idea to check if you are retracing accidentally.\n\nWhen is a\n\ntf.function\n\ntracing?\n\nTo figure out when your\n\ntf.function\n\nis tracing, add a\n\nprint\n\nstatement to its code. As a rule of thumb,\n\ntf.function\n\nwill execute the\n\nprint\n\nstatement every time it traces.\n\n@tf\n\n.\n\nfunction\n\ndef\n\na_function_with_python_side_effect\n\n(\n\nx\n\n):\n\nprint\n\n(\n\n\"Tracing!\"\n\n)\n\n# An eager-only side effect.\n\nreturn\n\nx\n\n*\n\nx\n\n+\n\ntf\n\n.\n\nconstant\n\n(\n\n2\n\n)\n\n# This is traced the first time.\n\nprint\n\n(\n\na_function_with_python_side_effect\n\n(\n\ntf\n\n.\n\nconstant\n\n(\n\n2\n\n)))\n\n# The second time through, you won't see the side effect.\n\nprint\n\n(\n\na_function_with_python_side_effect\n\n(\n\ntf\n\n.\n\nconstant\n\n(\n\n3\n\n)))\n\nTracing!\ntf.Tensor(6, shape=(), dtype=int32)\ntf.Tensor(11, shape=(), dtype=int32)\n\n# This retraces each time the Python argument changes,\n\n# as a Python argument could be an epoch count or other\n\n# hyperparameter.\n\nprint\n\n(\n\na_function_with_python_side_effect\n\n(\n\n2\n\n))\n\nprint\n\n(\n\na_function_with_python_side_effect\n\n(\n\n3\n\n))\n\nTracing!\ntf.Tensor(6, shape=(), dtype=int32)\nTracing!\ntf.Tensor(11, shape=(), dtype=int32)\n\nNew Python arguments always trigger the creation of a new graph, hence the extra tracing.\n\nNext steps\n\nYou can learn more about\n\ntf.function\n\non the API reference page and by following the\n\nBetter performance with\n\ntf.function\n\nguide.",
  "metadata": {
    "character_count": 39361,
    "word_count": 5195
  }
}