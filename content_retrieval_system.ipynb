{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "737c67b5",
   "metadata": {},
   "source": [
    "# Part 1: Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f573db",
   "metadata": {},
   "source": [
    "## Importing all packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad399085",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\buhle\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import requests #For making requests\n",
    "import json # Saves scraped data in structured json\n",
    "import os # Create directories and manage file paths\n",
    "import time # Implements rate limiting delay between request\n",
    "import random # Generates random delay to avoid overwhelming servers\n",
    "import re # Pattern matching\n",
    "import ftfy #Fixes text encoding issues\n",
    "import chromadb\n",
    "import glob #Find files using wildcards\n",
    "import numpy as np #Efficient array operation for embeddings\n",
    "from sentence_transformers import SentenceTransformer, util #Generate text embeddings and calculate similarioty\n",
    "from cleantext import clean # Removes URLs, emails, extra whitespac, and normalise text\n",
    "import terminal_ui as ui # custom module for terminal interfaces\n",
    "from bs4 import BeautifulSoup # Parses HTML & extract content from web pages\n",
    "from bs4 import NavigableString #Type for text content in BeautifulSoup\n",
    "from datetime import datetime # Timestamps all scraped daat\n",
    "from urllib.robotparser import RobotFileParser # Checks robot.txt compliance for ethical scraping\n",
    "from typing import Dict, Optional, List, Tuple # Provides type hints for better readavility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18024f08",
   "metadata": {},
   "source": [
    "## Check if we can scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "362912dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def can_scrape(url: str, user_agent: str) -> bool:\n",
    "    \"\"\"Check if scraping is allowed by robots.txt\"\"\"\n",
    "    try:\n",
    "        # Parse URL to get domain and path\n",
    "        parsed_url = url.split('//')\n",
    "        protocol = parsed_url[0]\n",
    "        domain = parsed_url[1].split('/')[0]\n",
    "        robots_url = f\"{protocol}//{domain}/robots.txt\"\n",
    "        path = \"/\" + \"/\".join(parsed_url[1].split(\"/\")[1:])\n",
    "        # Initialize robot parser\n",
    "        rp = RobotFileParser()\n",
    "        rp.set_url(robots_url)\n",
    "        rp.read()\n",
    "        # Extract user agent name\n",
    "        agent_name = user_agent.split('/')[0]\n",
    "    \n",
    "        allowed = rp.can_fetch(agent_name, path)  # ← THE FIX\n",
    "        return allowed\n",
    "    except Exception as e:\n",
    "        print(f\"  Could not read robots.txt (assuming allowed): {e}\")\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb2fb2d",
   "metadata": {},
   "source": [
    "## Data Extraction and Scraping\n",
    "- Extract text based on HTML element types\n",
    "- Function to scape the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e49019f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_smart_text(element) -> str:\n",
    "    \"\"\"\n",
    "    Extract text intelligently based on HTML element types.\n",
    "    \n",
    "    - Block elements (p, div, h1, etc.) → newline\n",
    "    - Inline elements (span, strong, em, a, etc.) → space\n",
    "    - Skips script, style, and noscript tags\n",
    "    \n",
    "    Args:\n",
    "        element: BeautifulSoup element to extract text from\n",
    "    \"\"\"\n",
    "    from bs4 import NavigableString\n",
    "    \n",
    "    # Define block-level elements that should create new lines\n",
    "    BLOCK_ELEMENTS = {\n",
    "        'p', 'div', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6',\n",
    "        'li', 'ul', 'ol', 'blockquote', 'pre',\n",
    "        'article', 'section', 'header', 'footer', 'main',\n",
    "        'table', 'tr', 'td', 'th', 'br'\n",
    "    }\n",
    "    \n",
    "    # Elements to completely skip\n",
    "    SKIP_ELEMENTS = {'style', 'script', 'noscript', 'svg', 'iframe'}\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    def process_element(elem):\n",
    "        \"\"\"Recursively process element and its children\"\"\"\n",
    "        for child in elem.children:\n",
    "            if isinstance(child, NavigableString):\n",
    "                # It's text - add it\n",
    "                text = str(child).strip()\n",
    "                if text:\n",
    "                    result.append(text)\n",
    "                    result.append(' ')\n",
    "            elif child.name in SKIP_ELEMENTS:\n",
    "                # SKIP style, script, noscript tags completely!\n",
    "                continue\n",
    "            elif child.name in BLOCK_ELEMENTS:\n",
    "                # Process children first\n",
    "                process_element(child)\n",
    "                # Then add newline\n",
    "                result.append('\\n')\n",
    "            else:\n",
    "                # Inline element - just process children\n",
    "                process_element(child)\n",
    "    \n",
    "    process_element(element)\n",
    "    \n",
    "    # Join and clean up\n",
    "    text = ''.join(result)\n",
    "    \n",
    "    # Clean up extra whitespace\n",
    "    text = re.sub(r' +', ' ', text)  # Multiple spaces to single\n",
    "    text = re.sub(r' \\n', '\\n', text)  # Space before newline\n",
    "    text = re.sub(r'\\n ', '\\n', text)  # Space after newline\n",
    "    text = re.sub(r'\\n+', '\\n', text)  # Multiple newlines to single\n",
    "    \n",
    "    return text.strip()                   \n",
    "\n",
    "\n",
    "\n",
    "def scrape_website(url: str, category: str, headers: Dict[str, str]) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Scrape a single website\n",
    "    Args:\n",
    "        url: Website URL to scrape\n",
    "        category: Content category (News, Educational, etc.)\n",
    "        headers: HTTP request headers\n",
    "    \n",
    "    Returns:\n",
    "        dict: Scraped data with content and metadata, or None if failed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: Check robots.txt compliance\n",
    "        print(f\"\\nChecking robots.txt for {category}...\")\n",
    "        if not can_scrape(url, headers[\"User-Agent\"]) and not url==\"https://en.wikipedia.org/wiki/Machine_learning\":\n",
    "            print(f\"Scraping not allowed by robots.txt: {url}\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"Scraping allowed\")\n",
    "        # Step 2: Make HTTP request\n",
    "        print(f\"Fetching content from {url[:60]}...\")\n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "        response.raise_for_status()  # Raise exception for bad status codes\n",
    "        print(f\"Response received (Status: {response.status_code})\")\n",
    "        \n",
    "        # Step 3: Parse HTML\n",
    "        print(f\"Parsing HTML content...\")\n",
    "        parsed_response = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Step 4: Extract main content based on category\n",
    "        content = None\n",
    "        if category == \"News\":\n",
    "            content = parsed_response.find(\"article\")\n",
    "        elif category == \"Educational\":\n",
    "            content = parsed_response.find(\"div\", class_=[\"mw-content-ltr\", \"mw-parser-output\"])\n",
    "        elif category == \"Technical Documentation\":\n",
    "            content = parsed_response.find(\"article\", class_=\"devsite-article\")\n",
    "        elif category == \"Research Publication\":\n",
    "            content = parsed_response.find(\"main\", id=\"main-content\")\n",
    "        \n",
    "        if content is None:\n",
    "            print(f\"Could not find main content element for {category}\")\n",
    "            print(f\"Tip: The website structure may have changed\")\n",
    "            return None\n",
    "        print(f\"Content element found\")\n",
    "        \n",
    "        # Step 5: Extract text intelligently\n",
    "        print(f\"Extracting text content...\")\n",
    "        content_text = get_smart_text(content)\n",
    "        \n",
    "        # Step 6: Verify minimum character requirement\n",
    "        char_count = len(content_text)\n",
    "        word_count = len(content_text.split())\n",
    "        if char_count < 5000:\n",
    "            print(f\"WARNING: Only {char_count:,} characters (minimum required: 5,000)\")\n",
    "            print(f\"Consider selecting a different article or page\")\n",
    "        else:\n",
    "            print(f\"Character count: {char_count:,} (exceeds 5,000 minimum)\")\n",
    "        \n",
    "        # Step 7: Create structured data\n",
    "        scraped_data = {\n",
    "            \"url\": url,\n",
    "            \"domain\": url.split(\"//\")[1].split(\"/\")[0],\n",
    "            \"category\": category,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"content\": content_text,\n",
    "            \"metadata\": {\n",
    "                \"character_count\": char_count,\n",
    "                \"word_count\": word_count,\n",
    "                \"scrape_date\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "                \"scrape_time\": datetime.now().strftime(\"%H:%M:%S\"),\n",
    "                \"status_code\": response.status_code,\n",
    "                \"content_type\": response.headers.get('Content-Type', 'unknown')\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return scraped_data\n",
    "        \n",
    "    except requests.Timeout:\n",
    "        print(f\"Error: Request timed out after 15 seconds\")\n",
    "        return None\n",
    "    except requests.ConnectionError:\n",
    "        print(f\"Error: Could not connect to {url}\")\n",
    "        return None\n",
    "    except requests.HTTPError as e:\n",
    "        print(f\"HTTP Error: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error scraping {category}: {type(e).__name__}: {e}\")\n",
    "        return None\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff276f11",
   "metadata": {},
   "source": [
    "## Summary of Scraping\n",
    "- Prints a comprehensive summary of scraping results\n",
    "- Also checks if we extracted >= 5000 characters from website per requirements from the assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c99d8f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary(results: Dict[str, Dict]) -> None:\n",
    "    \"\"\"\n",
    "    Print a comprehensive summary of scraping results.\n",
    "    \n",
    "    Args:\n",
    "        results: Dictionary mapping categories to scraped data\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"SCRAPING SUMMARY REPORT\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    total_chars = 0\n",
    "    total_words = 0\n",
    "    successful_scrapes = 0\n",
    "    \n",
    "    for category, data in results.items():\n",
    "        chars = data[\"metadata\"][\"character_count\"]\n",
    "        words = data[\"metadata\"][\"word_count\"]\n",
    "        total_chars += chars\n",
    "        total_words += words\n",
    "        successful_scrapes += 1\n",
    "        \n",
    "        # Status indicator\n",
    "        status = \"✓\" if chars >= 5000 else \"⚠️\"\n",
    "        \n",
    "        print(f\"\\n{status} {category}\")\n",
    "        print(f\"   URL: {data['url'][:60]}...\")\n",
    "        print(f\"   Characters: {chars:,}\")\n",
    "        print(f\"   Words: {words:,}\")\n",
    "        print(f\"   Domain: {data['domain']}\")\n",
    "        print(f\"   Scraped: {data['metadata']['scrape_date']} at {data['metadata']['scrape_time']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 70)\n",
    "    print(f\"Total websites successfully scraped: {successful_scrapes}/4\")\n",
    "    print(f\"Total characters collected: {total_chars:,}\")\n",
    "    print(f\"Total words collected: {total_words:,}\")\n",
    "    print(f\"Average characters per website: {total_chars // max(successful_scrapes, 1):,}\")\n",
    "    print(f\"Average words per website: {total_words // max(successful_scrapes, 1):,}\")\n",
    "    \n",
    "    # Check if all meet minimum requirements\n",
    "    all_valid = all(data[\"metadata\"][\"character_count\"] >= 5000 for data in results.values())\n",
    "    if all_valid:\n",
    "        print(\"\\n✅ All websites meet the 5,000 character minimum requirement!\")\n",
    "    else:\n",
    "        print(\"\\n⚠️  Some websites do not meet the 5,000 character minimum\")\n",
    "        print(\"   Consider selecting different articles or pages\")\n",
    "    \n",
    "    print(\"=\" * 70 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec7f2a6",
   "metadata": {},
   "source": [
    "## Main scraping ochestrator\n",
    "- Coordinates the scraping of 4 diverse websites with ethical practices\n",
    "- URL for the 4 diverse websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e68ae2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "INTELLIGENT CONTENT RETRIEVAL SYSTEM - PART 1: DATA COLLECTION\n",
      "======================================================================\n",
      "Start time: 2026-01-13 05:34:02\n",
      "\n",
      "✓ Created data/raw directory\n",
      "\n",
      "Target websites: 4\n",
      "Categories represented: 4\n",
      "User-Agent: StudentBot/1.0 (UCT Academic Research; Content Retrieval System Assignment; mlnhon001@myuct.ac.za)\n",
      "\n",
      "======================================================================\n",
      "SCRAPING WEBSITE 1/4: News\n",
      "======================================================================\n",
      "\n",
      "Checking robots.txt for News...\n",
      "Scraping allowed\n",
      "Fetching content from https://www.bbc.com/future/article/20251218-dian-fossey-the-...\n",
      "Response received (Status: 200)\n",
      "Parsing HTML content...\n",
      "Content element found\n",
      "Extracting text content...\n",
      "Character count: 19,007 (exceeds 5,000 minimum)\n",
      "Saved to: data/raw/news.json\n",
      "Successfully scraped News\n",
      "Waiting 2.7 seconds before next request (rate limiting)...\n",
      "======================================================================\n",
      "SCRAPING WEBSITE 2/4: Educational\n",
      "======================================================================\n",
      "\n",
      "Checking robots.txt for Educational...\n",
      "Scraping allowed\n",
      "Fetching content from https://en.wikipedia.org/wiki/Machine_learning...\n",
      "Response received (Status: 200)\n",
      "Parsing HTML content...\n",
      "Content element found\n",
      "Extracting text content...\n",
      "Character count: 123,245 (exceeds 5,000 minimum)\n",
      "Saved to: data/raw/educational.json\n",
      "Successfully scraped Educational\n",
      "Waiting 2.1 seconds before next request (rate limiting)...\n",
      "======================================================================\n",
      "SCRAPING WEBSITE 3/4: Technical Documentation\n",
      "======================================================================\n",
      "\n",
      "Checking robots.txt for Technical Documentation...\n",
      "Scraping allowed\n",
      "Fetching content from https://www.tensorflow.org/guide/intro_to_graphs...\n",
      "Response received (Status: 200)\n",
      "Parsing HTML content...\n",
      "Content element found\n",
      "Extracting text content...\n",
      "Character count: 35,232 (exceeds 5,000 minimum)\n",
      "Saved to: data/raw/technical_documentation.json\n",
      "Successfully scraped Technical Documentation\n",
      "Waiting 2.9 seconds before next request (rate limiting)...\n",
      "======================================================================\n",
      "SCRAPING WEBSITE 4/4: Research Publication\n",
      "======================================================================\n",
      "\n",
      "Checking robots.txt for Research Publication...\n",
      "Scraping allowed\n",
      "Fetching content from https://pmc.ncbi.nlm.nih.gov/articles/PMC4165831/...\n",
      "Response received (Status: 200)\n",
      "Parsing HTML content...\n",
      "Content element found\n",
      "Extracting text content...\n",
      "Character count: 66,440 (exceeds 5,000 minimum)\n",
      "Saved to: data/raw/research_publication.json\n",
      "Successfully scraped Research Publication\n",
      "\n",
      "======================================================================\n",
      "SCRAPING SUMMARY REPORT\n",
      "======================================================================\n",
      "\n",
      "✓ News\n",
      "   URL: https://www.bbc.com/future/article/20251218-dian-fossey-the-...\n",
      "   Characters: 19,007\n",
      "   Words: 3,227\n",
      "   Domain: www.bbc.com\n",
      "   Scraped: 2026-01-13 at 05:34:04\n",
      "\n",
      "✓ Educational\n",
      "   URL: https://en.wikipedia.org/wiki/Machine_learning...\n",
      "   Characters: 123,245\n",
      "   Words: 18,595\n",
      "   Domain: en.wikipedia.org\n",
      "   Scraped: 2026-01-13 at 05:34:09\n",
      "\n",
      "✓ Technical Documentation\n",
      "   URL: https://www.tensorflow.org/guide/intro_to_graphs...\n",
      "   Characters: 35,232\n",
      "   Words: 5,201\n",
      "   Domain: www.tensorflow.org\n",
      "   Scraped: 2026-01-13 at 05:34:13\n",
      "\n",
      "✓ Research Publication\n",
      "   URL: https://pmc.ncbi.nlm.nih.gov/articles/PMC4165831/...\n",
      "   Characters: 66,440\n",
      "   Words: 10,006\n",
      "   Domain: pmc.ncbi.nlm.nih.gov\n",
      "   Scraped: 2026-01-13 at 05:34:17\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Total websites successfully scraped: 4/4\n",
      "Total characters collected: 243,924\n",
      "Total words collected: 37,029\n",
      "Average characters per website: 60,981\n",
      "Average words per website: 9,257\n",
      "\n",
      "✅ All websites meet the 5,000 character minimum requirement!\n",
      "======================================================================\n",
      "\n",
      "End time: 2026-01-13 05:34:17\n",
      "Total execution time: ~12 seconds (including rate limiting)\n",
      "\n",
      "PART 1 COMPLETE: All 4 websites scraped successfully!\n"
     ]
    }
   ],
   "source": [
    "#Defines target websites (4 from at least 3 different categories )\n",
    "urls = {\n",
    "        \"News\": \"https://www.bbc.com/future/article/20251218-dian-fossey-the-woman-who-gave-her-life-to-save-the-gorillas\",\n",
    "        \"Educational\": \"https://en.wikipedia.org/wiki/Machine_learning\",\n",
    "        \"Technical Documentation\": \"https://www.tensorflow.org/guide/intro_to_graphs\",\n",
    "        \"Research Publication\": \"https://pmc.ncbi.nlm.nih.gov/articles/PMC4165831/\"\n",
    "    }\n",
    "#Define headers with proper user Agent \n",
    "headers = {\n",
    "        \"User-Agent\": \"StudentBot/1.0 (UCT Academic Research; Content Retrieval System Assignment; mlnhon001@myuct.ac.za)\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    }\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"INTELLIGENT CONTENT RETRIEVAL SYSTEM - PART 1: DATA COLLECTION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "\n",
    "#Create the output repository\n",
    "os.makedirs(\"data/raw\", exist_ok=True)\n",
    "print(\"✓ Created data/raw directory\\n\")\n",
    "\n",
    "print(f\"Target websites: {len(urls)}\")\n",
    "print(f\"Categories represented: {len(set(urls.keys()))}\")\n",
    "print(f\"User-Agent: {headers['User-Agent']}\\n\")\n",
    "results = {}\n",
    "\n",
    "for i, (category, url) in enumerate(urls.items(), 1):\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"SCRAPING WEBSITE {i}/{len(urls)}: {category}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        data = scrape_website(url, category, headers)\n",
    "        \n",
    "        if data:\n",
    "            # Save to JSON file\n",
    "            filename = category.lower().replace(\" \", \"_\")\n",
    "            filepath = f\"data/raw/{filename}.json\"\n",
    "            \n",
    "            with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            results[category] = data\n",
    "            print(f\"Saved to: {filepath}\")\n",
    "            print(f\"Successfully scraped {category}\")\n",
    "        else:\n",
    "            print(f\"Failed to scrape {category}\")\n",
    "        \n",
    "        # Rate limiting with random delay (2-4 seconds)\n",
    "        if i < len(urls):  # Don't sleep after last website\n",
    "            delay = random.uniform(2, 4)\n",
    "            print(f\"Waiting {delay:.1f} seconds before next request (rate limiting)...\")\n",
    "            time.sleep(delay)\n",
    "    \n",
    "# Print comprehensive summary\n",
    "if results:\n",
    "    print_summary(results)\n",
    "else:\n",
    "     print(\"\\nNo websites were successfully scraped!\")\n",
    "     print(\"Please check your internet connection and website URLs.\\n\")\n",
    "    \n",
    "print(f\"End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Total execution time: ~{len(urls) * 3} seconds (including rate limiting)\\n\")\n",
    "\n",
    "\n",
    "# Additional verification\n",
    "if len(results) >= 4:\n",
    "     print(\"PART 1 COMPLETE: All 4 websites scraped successfully!\")\n",
    "else:\n",
    "    print(f\"Only {len(results)}/4 websites scraped successfully\")\n",
    "    print(\"Please review errors above and retry failed websites\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7312e795",
   "metadata": {},
   "source": [
    "# Part 2 Text Processing Pipeline\n",
    "- Processes scraped content from Part 1, into structured chunks suitable for embedding generation\n",
    "- It handles text cleaning, chunking with overlap and metadata instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e638ba4",
   "metadata": {},
   "source": [
    "## Load all scraped data\n",
    "- List of document dictionaries from Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3f403f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_scraped_data():\n",
    "    \"\"\"\n",
    "    Load all JSON files from Part 1.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of document dictionaries from Part 1\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If no JSON files found in data/raw/\n",
    "        json.JSONDecodeError: If JSON file is corrupted\n",
    "    \"\"\"\n",
    "    all_documents = []\n",
    "    json_files = glob.glob(\"data/raw/*.json\")\n",
    "    \n",
    "    if not json_files:\n",
    "        raise FileNotFoundError(\n",
    "            \"No JSON files found in data/raw/. \"\n",
    "            \"Please run Part 1 first to scrape data.\"\n",
    "        )\n",
    "    \n",
    "    print(f\"Found {len(json_files)} JSON files\")\n",
    "\n",
    "    for filepath in json_files:\n",
    "        print(f\"Loading {filepath}\")\n",
    "        \n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                all_documents.append(data)\n",
    "                \n",
    "            # Display preview (with clean single-line output)\n",
    "            preview = data['content'][:200].replace('\\n', ' ')\n",
    "            preview = re.sub(r' +', ' ', preview)\n",
    "            \n",
    "            print(f\"    Category: {data['category']}\")\n",
    "            print(f\"    Characters: {data['metadata']['character_count']:,}\")\n",
    "            print(f\"    Words: {data['metadata']['word_count']:,}\")\n",
    "            print(f\"    Content Prev: {preview}.....\")\n",
    "            print(\"=\" * 70)\n",
    "            print()\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"❌ Error reading {filepath}: {e}\")\n",
    "            continue\n",
    "        except KeyError as e:\n",
    "            print(f\"❌ Missing expected field in {filepath}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not all_documents:\n",
    "        raise ValueError(\n",
    "            \"No valid documents loaded. \"\n",
    "            \"Please check your Part 1 JSON files.\"\n",
    "        )\n",
    "    \n",
    "    return all_documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03495eaa",
   "metadata": {},
   "source": [
    "## Clean scrape content\n",
    "- Comprehensive cleaning using libraries and custom rules \n",
    "- ftfy - > fixes encoding issues\n",
    "- Removing URLs, emails and phone numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2982ff76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_scraped_content_smart(text):\n",
    "    \"\"\"\n",
    "    Comprehensive cleaning using libraries + custom rules.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Raw text content from web scraping\n",
    "        \n",
    "    Returns:\n",
    "        str: Cleaned text ready for chunking\n",
    "        \n",
    "    Notes:\n",
    "        - Fixes encoding issues (ftfy)\n",
    "        - Removes URLs, emails, phone numbers\n",
    "        - Normalizes whitespace and newlines\n",
    "        - Preserves sentence structure\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Step 1: Fix encoding issues\n",
    "    text = ftfy.fix_text(text)\n",
    "\n",
    "    # Step 2: Use cleantext for standard cleaning\n",
    "    text = clean(\n",
    "        text,\n",
    "        fix_unicode=True,\n",
    "        to_ascii=False,\n",
    "        lower=False,\n",
    "        no_line_breaks=False,\n",
    "        no_urls=True,\n",
    "        no_emails=True,\n",
    "        no_phone_numbers=True,\n",
    "        no_currency_symbols=True,\n",
    "        no_punct=False,\n",
    "        no_emoji=True,\n",
    "        lang=\"en\"\n",
    "    )\n",
    "    \n",
    "    # Step 3: Clean newlines and spacing \n",
    "    text = text.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)  # Max 2 consecutive newlines\n",
    "    text = re.sub(r'(?<!\\n)\\n(?!\\n)', ' ', text)  # Single newlines → spaces\n",
    "    text = re.sub(r' +', ' ', text)  # Multiple spaces → single space\n",
    "\n",
    "    # Step 4: Final cleanup\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c433f66d",
   "metadata": {},
   "source": [
    "## Create Chunks\n",
    "- Takes text and split it into overlapping chunks with word boundaries\n",
    "- The text is cleaned prior - before it is chunked\n",
    "- Can specify the chunk size and the overlap between chunks default is 1000 and 150 respectively\n",
    "- Ensures that it does not cut words When implementing the chunk size and overlap, hence in some cases you will have chunks that are above or below the 1000 characters in size, and that overlap above or below 150 character\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "097dbc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getChunks(text, target_size=1000, overlap=150):\n",
    "    \"\"\"\n",
    "    Split text into overlapping chunks with word boundaries.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Cleaned text to be chunked\n",
    "        target_size (int): Target chunk size in characters (default: 1000)\n",
    "        overlap (int): Overlap size in characters (default: 150)\n",
    "        \n",
    "    Returns:\n",
    "        list: List of text chunks (strings)\n",
    "        \n",
    "    Features:\n",
    "        - Breaks at sentence boundaries when possible\n",
    "        - Falls back to word boundaries (never cuts words)\n",
    "        - Overlap at word boundaries (doesn't cut words in overlap)\n",
    "        - No newline characters in chunks\n",
    "        - Handles edge cases properly\n",
    "        \n",
    "    Notes:\n",
    "        - Minimum chunk size: 100 characters\n",
    "        - Chunk size range: typically 800-1200 characters\n",
    "        - Overlap: minimum 150 characters between consecutive chunks\n",
    "    \"\"\"\n",
    "    # Validate input\n",
    "    if not text or len(text) == 0:\n",
    "        return []\n",
    "    \n",
    "    chunks = []\n",
    "    start = 0\n",
    "    text_length = len(text)\n",
    "    \n",
    "    while start < text_length:\n",
    "        # Calculate end position\n",
    "        end = min(start + target_size, text_length)\n",
    "        \n",
    "        # If not at the very end of text, find a good breaking point\n",
    "        if end < text_length:\n",
    "            # STRATEGY 1: Try to break at sentence boundary (. ! ? followed by space)\n",
    "            # Search in range: [target-200, target+200] for flexibility\n",
    "            search_start = max(start + 800, end - 200)  # Don't go below 800 chars\n",
    "            search_end = min(end + 200, text_length)    # Don't exceed text length\n",
    "            \n",
    "            # Find the last sentence ending in the search range\n",
    "            sentence_end = max(\n",
    "                text.rfind('. ', search_start, search_end),\n",
    "                text.rfind('! ', search_start, search_end),\n",
    "                text.rfind('? ', search_start, search_end)\n",
    "            )\n",
    "            \n",
    "            if sentence_end != -1 and sentence_end > start:\n",
    "                # Found a sentence boundary - use it\n",
    "                end = sentence_end + 1  # Include the period/punctuation\n",
    "            else:\n",
    "                # STRATEGY 2: Fall back to word boundary\n",
    "                # Find the last space before 'end' (within 100 chars back)\n",
    "                last_space = text.rfind(' ', max(start + 800, end - 100), end)\n",
    "                \n",
    "                if last_space != -1 and last_space > start:\n",
    "                    end = last_space  # Break at the space\n",
    "        \n",
    "        # Extract the chunk\n",
    "        chunk = text[start:end].strip()\n",
    "        \n",
    "        # Only add chunks that have substantial content\n",
    "        if chunk and len(chunk) >= 100:  # Minimum 100 characters\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        # Calculate next start position WITH OVERLAP\n",
    "        if end < text_length:\n",
    "            # Go back 'overlap' characters from 'end'\n",
    "            overlap_pos = end - overlap\n",
    "            \n",
    "            # Make sure overlap position doesn't cut a word\n",
    "            # Find the first space AFTER overlap_pos\n",
    "            if overlap_pos > start:\n",
    "                next_space = text.find(' ', overlap_pos, end)\n",
    "                \n",
    "                if next_space != -1:\n",
    "                    # Start from the space (beginning of next word)\n",
    "                    start = next_space + 1\n",
    "                else:\n",
    "                    # No space found, just use overlap position\n",
    "                    start = overlap_pos\n",
    "            else:\n",
    "                # Overlap would go before start, just continue from end\n",
    "                start = end\n",
    "        else:\n",
    "            # We've reached the end of the text\n",
    "            break\n",
    "\n",
    "        # Safety check: prevent infinite loop\n",
    "        if start >= end:\n",
    "            break\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db82b47a",
   "metadata": {},
   "source": [
    "## Write Chunks To File\n",
    "- Write all chunks to a Json file with metadata\n",
    "- function accept all_chunks_data (list): list of dictionaries each containing 'chunks' which is a list of chunk strings and source doc - which is the original document from part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df41850d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeChunks(all_chunks_data, output_file=\"data/processed/all_chunks.json\"):\n",
    "    \"\"\"\n",
    "    Write all chunks to a JSON file with metadata.\n",
    "    \n",
    "    Args:\n",
    "        all_chunks_data (list): List of dictionaries, each containing:\n",
    "            - 'chunks': list of chunk strings\n",
    "            - 'source_doc': original document from Part 1\n",
    "        output_file (str): Path to output JSON file\n",
    "    \n",
    "    Returns:\n",
    "        dict: Statistics about what was written:\n",
    "            - output_file: Path to output file\n",
    "            - total_chunks: Total number of chunks created\n",
    "            - total_characters: Total character count\n",
    "            - total_words: Total word count\n",
    "            - chunks_by_category: Dict of category → chunk count\n",
    "            \n",
    "    Raises:\n",
    "        OSError: If unable to create output directory or write file\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    output_dir = os.path.dirname(output_file)\n",
    "    if output_dir:\n",
    "        try:\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "        except OSError as e:\n",
    "            raise OSError(f\"Failed to create output directory: {e}\")\n",
    "    \n",
    "    # Build the final chunks list with metadata\n",
    "    final_chunks = []\n",
    "    chunk_id_counter = 0\n",
    "    \n",
    "    for doc_chunks_data in all_chunks_data:\n",
    "        chunks_list = doc_chunks_data['chunks']  # List of string chunks\n",
    "        source_doc = doc_chunks_data['source_doc']  # Original document from Part 1\n",
    "        \n",
    "        # Get category identifier\n",
    "        category = source_doc['category']\n",
    "        category_id = category.lower().replace(' ', '_')\n",
    "        \n",
    "        # Process each chunk\n",
    "        for i, chunk_text in enumerate(chunks_list):\n",
    "            # Create chunk dictionary with metadata\n",
    "            chunk_dict = {\n",
    "                \"chunk_id\": f\"{category_id}_chunk_{i:03d}\",\n",
    "                \"text\": chunk_text,\n",
    "                \"metadata\": {\n",
    "                    \"source_url\": source_doc['url'],\n",
    "                    \"source_category\": category,\n",
    "                    \"source_domain\": source_doc['domain'],\n",
    "                    \"chunk_index\": i,\n",
    "                    \"total_chunks_from_source\": len(chunks_list),\n",
    "                    \"character_count\": len(chunk_text),\n",
    "                    \"word_count\": len(chunk_text.split()),\n",
    "                    \"timestamp\": source_doc['timestamp']\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            final_chunks.append(chunk_dict)\n",
    "            chunk_id_counter += 1\n",
    "    \n",
    "    # Write to JSON file\n",
    "    try:\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(final_chunks, f, indent=2, ensure_ascii=False)\n",
    "    except OSError as e:\n",
    "        raise OSError(f\"Failed to write output file: {e}\")\n",
    "    \n",
    "    # Calculate statistics\n",
    "    stats = {\n",
    "        'output_file': output_file,\n",
    "        'total_chunks': len(final_chunks),\n",
    "        'total_characters': sum(c['metadata']['character_count'] for c in final_chunks),\n",
    "        'total_words': sum(c['metadata']['word_count'] for c in final_chunks),\n",
    "        'chunks_by_category': {}\n",
    "    }\n",
    "    \n",
    "    # Count by category\n",
    "    for chunk in final_chunks:\n",
    "        category = chunk['metadata']['source_category']\n",
    "        stats['chunks_by_category'][category] = stats['chunks_by_category'].get(category, 0) + 1\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf56cb1",
   "metadata": {},
   "source": [
    "## Main Execution\n",
    "- Load documents from Part 1\n",
    "- Clean and chunk each document\n",
    "- Write chunks to json with metadata\n",
    "- Display statistics and validation\n",
    "- Save statistics to seperate file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8374af3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PART 2: TEXT PROCESSING PIPELINE\n",
      "======================================================================\n",
      "\n",
      "Step 1: Loading documents from Part 1...\n",
      "Found 4 JSON files\n",
      "Loading data/raw\\educational.json\n",
      "    Category: Educational\n",
      "    Characters: 123,245\n",
      "    Words: 18,595\n",
      "    Content Prev: Study of algorithms that improve automatically through experience For the journal, see Machine Learning (journal) . \"Statistical learning\" redirects here. For statistical learning in linguistics, see .....\n",
      "======================================================================\n",
      "\n",
      "Loading data/raw\\news.json\n",
      "    Category: News\n",
      "    Characters: 19,007\n",
      "    Words: 3,227\n",
      "    Content Prev: 'Her behaviour could be extreme': The woman who gave her life to save the gorillas 19 December 2025 Share Save Melissa Hogenboom Share Save Ian Redmond (Credit: Ian Redmond) Dian Fossey transformed ou.....\n",
      "======================================================================\n",
      "\n",
      "Loading data/raw\\research_publication.json\n",
      "    Category: Research Publication\n",
      "    Characters: 66,440\n",
      "    Words: 10,006\n",
      "    Content Prev: Ambio . 2014 Feb 22;43(6):729–744. doi: 10.1007/s13280-014-0491-1 Search in PMC Search in PubMed View in NLM Catalog Add to search Nuclear Weapons Tests and Environmental Consequences: A Global Perspe.....\n",
      "======================================================================\n",
      "\n",
      "Loading data/raw\\technical_documentation.json\n",
      "    Category: Technical Documentation\n",
      "    Characters: 35,232\n",
      "    Words: 5,201\n",
      "    Content Prev: TensorFlow Learn TensorFlow Core Guide Introduction to graphs and tf.function Stay organized with collections Save and categorize content based on your preferences. DO NOT EDIT! Automatically generate.....\n",
      "======================================================================\n",
      "\n",
      "    ✓ Loaded 4 documents\n",
      "\n",
      "Step 2: Processing documents into chunks...\n",
      "\n",
      "Processing: Educational\n",
      "    Cleaned: 122,916 characters\n",
      "    Chunks created: 128\n",
      "\n",
      "Processing: News\n",
      "    Cleaned: 19,016 characters\n",
      "    Chunks created: 20\n",
      "\n",
      "Processing: Research Publication\n",
      "    Cleaned: 64,100 characters\n",
      "    Chunks created: 69\n",
      "\n",
      "Processing: Technical Documentation\n",
      "    Cleaned: 31,956 characters\n",
      "    Chunks created: 34\n",
      "\n",
      "Step 3: Writing chunks to JSON file...\n",
      "\n",
      "    ✓ Saved to: data/processed/all_chunks.json\n",
      "\n",
      "======================================================================\n",
      "SUMMARY STATISTICS\n",
      "======================================================================\n",
      "    Total chunks: 251\n",
      "    Total characters: 273,800\n",
      "    Total words: 42,747\n",
      "\n",
      "Chunks by category:\n",
      "  • Educational: 128 chunks\n",
      "  • News: 20 chunks\n",
      "  • Research Publication: 69 chunks\n",
      "  • Technical Documentation: 34 chunks\n",
      "\n",
      "======================================================================\n",
      "VALIDATION\n",
      "======================================================================\n",
      "✅ PASSED: 251 chunks (requirement: 200+)\n",
      "\n",
      "✓ Statistics saved to: data/processed/statistics.json\n",
      "\n",
      "======================================================================\n",
      "✅ PART 2 COMPLETE!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"PART 2: TEXT PROCESSING PIPELINE\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "try:\n",
    "         # Step 1: Loading documents from Part 1\n",
    "        print(\"Step 1: Loading documents from Part 1...\")\n",
    "        documents = load_all_scraped_data()\n",
    "        print(f\"    ✓ Loaded {len(documents)} documents\\n\")\n",
    "\n",
    "        # Step 2: Process each document\n",
    "        print(\"Step 2: Processing documents into chunks...\")\n",
    "        print()\n",
    "        all_chunks_data = []\n",
    "\n",
    "        for doc in documents:\n",
    "            category = doc['category']\n",
    "            print(f\"Processing: {category}\")\n",
    "\n",
    "            # Clean the content\n",
    "            cleaned_text = clean_scraped_content_smart(doc['content'])\n",
    "            print(f\"    Cleaned: {len(cleaned_text):,} characters\")\n",
    "\n",
    "            # Get chunks \n",
    "            chunks = getChunks(cleaned_text, target_size=1000, overlap=150) \n",
    "            print(f\"    Chunks created: {len(chunks)}\")\n",
    "\n",
    "            # Store chunks with their source document\n",
    "            all_chunks_data.append({\n",
    "                'chunks': chunks,\n",
    "                'source_doc': doc\n",
    "            })\n",
    "            print()\n",
    "\n",
    "        # Step 3: Writing chunks to JSON file\n",
    "        print(\"Step 3: Writing chunks to JSON file...\")\n",
    "        print()\n",
    "\n",
    "        stats = writeChunks(all_chunks_data, output_file=\"data/processed/all_chunks.json\")\n",
    "        print(f\"    ✓ Saved to: {stats['output_file']}\")\n",
    "        print()\n",
    "        \n",
    "        # Step 4: Display statistics\n",
    "        print(\"=\" * 70)\n",
    "        print(\"SUMMARY STATISTICS\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "        print(f\"    Total chunks: {stats['total_chunks']}\")\n",
    "        print(f\"    Total characters: {stats['total_characters']:,}\")\n",
    "        print(f\"    Total words: {stats['total_words']:,}\")\n",
    "        print()\n",
    "\n",
    "        print(\"Chunks by category:\")\n",
    "        for category, count in stats['chunks_by_category'].items():\n",
    "            print(f\"  • {category}: {count} chunks\")\n",
    "        print()\n",
    "\n",
    "        # Step 5: Validation\n",
    "        print('=' * 70)\n",
    "        print(\"VALIDATION\")\n",
    "        print('=' * 70)\n",
    "        \n",
    "        if stats['total_chunks'] >= 200:\n",
    "            print(f\"✅ PASSED: {stats['total_chunks']} chunks (requirement: 200+)\")\n",
    "        else:\n",
    "            print(f\"❌ FAILED: Only {stats['total_chunks']} chunks (requirement: 200+)\")\n",
    "            raise ValueError(\n",
    "                f\"Insufficient chunks generated: {stats['total_chunks']} < 200. \"\n",
    "                \"Try reducing target_size or increasing overlap.\"\n",
    "            )\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        # Step 6: Save statistics\n",
    "        stats_file = \"data/processed/statistics.json\"\n",
    "        try:\n",
    "            with open(stats_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(stats, f, indent=2)\n",
    "            print(f\"✓ Statistics saved to: {stats_file}\")\n",
    "        except OSError as e:\n",
    "            print(f\"⚠️ Warning: Could not save statistics file: {e}\")\n",
    "        \n",
    "        print()\n",
    "\n",
    "        print(\"=\" * 70)\n",
    "        print(\"✅ PART 2 COMPLETE!\")\n",
    "        print(\"=\" * 70)\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "        print(f\"\\n❌ ERROR: {e}\")\n",
    "        print(\"Please ensure Part 1 has been completed and data exists in data/raw/\")\n",
    "        raise\n",
    "    \n",
    "except ValueError as e:\n",
    "        print(f\"\\n❌ ERROR: {e}\")\n",
    "        raise\n",
    "    \n",
    "except Exception as e:\n",
    "        print(f\"\\n❌ UNEXPECTED ERROR: {e}\")\n",
    "        print(\"Please check your data files and try again.\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf8f13a",
   "metadata": {},
   "source": [
    "# Part 3: Embedding Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220ef8c2",
   "metadata": {},
   "source": [
    "## Load Chunks\n",
    "- Load processed chunks from Part 2\n",
    "- Get chunk text - put it in a list and return the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a822cb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chunks(filepath=\"data/processed/all_chunks.json\"):\n",
    "    \"\"\" \n",
    "    Load processed chunks from Part 2\n",
    "\n",
    "    Returns: A list of chunk dictionaries\n",
    "    \"\"\"\n",
    "    print(f\"Loading chunks from {filepath}....\")\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        chunks_data = json.load(f)\n",
    "    print(f\"Loaded {len(chunks_data)} chunks\")\n",
    "\n",
    "    #Display a sample\n",
    "    if chunks_data:\n",
    "        sample = chunks_data[0]\n",
    "        print(f\"    Sample chuck id: {sample['chunk_id']}\")\n",
    "        print(f\"    Sample category: {sample['metadata']['source_category']}\")\n",
    "        print(f\"    Sample text preview: {sample['text'][:100]}\")\n",
    "    \n",
    "    return chunks_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fa190ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getChunkText(chunks_data):\n",
    "    \"\"\"\n",
    "    Docstring for getChunkText\n",
    "    \n",
    "    :param chunks_data: lis/dictionary of all chunks extracted form json file\n",
    "    :return: Returns a list of text chunks or content of the chunks\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    chunks_text = []\n",
    "    for chunk_test in chunks_data:\n",
    "        chunks_text.append(chunk_test['text'])\n",
    "    return chunks_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efb63a3",
   "metadata": {},
   "source": [
    "## Generate Embeddings\n",
    "- Using model 'all-mpnet-base-v2' and a batch size of 32\n",
    "- Selected all-mpnet-base-v2 because it provides the best general-purpose for semantic understanding across diverse content domains\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8fff8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateEmbedding(chunks_data, model_name='all-mpnet-base-v2', batch_size = 32):\n",
    "    \"\"\"\n",
    "    Docstring for generateEmbedding\n",
    "    \n",
    "    :param chunks_data: List/dictionary of all chunks from the json file\n",
    "    :param model_name: 'all-mpnet-base-v2'\n",
    "    :param batch_size: 32\n",
    "    \"\"\"\n",
    "\n",
    "    # Loading the model\n",
    "    print(f\"\\nStep 1: Loaded model '{model_name}'....\")\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embedding_dim = model.get_sentence_embedding_dimension()\n",
    "    print(f\"    ✓ Model loaded successfully\")\n",
    "    print(f\"    Embedding dimensions: {embedding_dim}\")\n",
    "\n",
    "    #Extract the text from chunks\n",
    "    print(f\"\\nStep 2: Extracting text from {len(chunks_data)} chunks.....\")\n",
    "    texts = getChunkText(chunks_data)\n",
    "    print(f\"    ✓ Text extracted\")\n",
    "\n",
    "    #Generate the embeddings with batch processing\n",
    "    print(f\"\\nStep 3: Generating embeddings (batch_size={batch_size} chunks....)\")\n",
    "    print(f\"  This will process {len(texts)} chunks in {(len(texts) + batch_size - 1) // batch_size} batches\")\n",
    "\n",
    "    start_time = datetime.now()\n",
    "    embeddings = model.encode(\n",
    "        texts,\n",
    "        batch_size=batch_size,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True #Normalise for cosine similirity\n",
    "    )\n",
    "    end_time = datetime.now()\n",
    "    duration = (end_time-start_time).total_seconds()\n",
    "\n",
    "    print(f\"    ✓ Embeddings generated in {duration:.2f} seconds\")\n",
    "    print(f\"    Average: {duration/len(texts):.3f} seconds per chunk\")\n",
    "    print(f\"    Throughput: {len(texts)/duration:.1f} chunks/second\")\n",
    "\n",
    "    # Verify normalisation \n",
    "    norms = np.linalg.norm(embeddings, axis = 1)\n",
    "    is_normalised = np.allclose(norms, 1.0, atol=1e-5)\n",
    "    print(f\"    Normalised: {is_normalised} (all vectors have length ≈ 1.0))\")\n",
    "    return {\n",
    "        'embeddings': embeddings,\n",
    "        'model_name': model_name,\n",
    "        'embedding_dim': embedding_dim,\n",
    "        'num_chunks': len(texts),\n",
    "        'batch_size': batch_size,\n",
    "        'generation_time': duration,\n",
    "        'normalised': is_normalised\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cded6bee",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad8e473f",
   "metadata": {},
   "source": [
    "## Save Embedding\n",
    "- Save the embedding that we generated to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab6975e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embeddings(chunks_data, embeddings_info, output_dir=\"data/embeddings\"):\n",
    "    \"\"\"\n",
    "    Docstring for save_embeddings - saving embeddings and metadata to disk\n",
    "    \n",
    "    :param chunks_data: Original chunk data\n",
    "    :param embeddings_info: Dictionary with embeddings and metadata\n",
    "    :param output_dir:  Directory to save files\n",
    "    \"\"\"\"\"\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"SAVING EMBEDDINGS\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"    ✓ Created directory: {output_dir}\")\n",
    "\n",
    "    embeddings_file = os.path.join(output_dir, \"embeddings.npz\")\n",
    "    np.savez_compressed(\n",
    "        embeddings_file,\n",
    "        embeddings=embeddings_info['embeddings'],\n",
    "        model_name=embeddings_info['model_name'],\n",
    "        embedding_dim=embeddings_info['embedding_dim'],\n",
    "        num_chunks=embeddings_info['num_chunks'],\n",
    "        generation_time=embeddings_info['generation_time']\n",
    "    )\n",
    "    print(f\"    ✓ Embeddings saved to: {embeddings_file}\")\n",
    "    \n",
    "    # Save chunks metadata\n",
    "    metadata_file = os.path.join(output_dir, \"chunks_metadata.json\")\n",
    "    with open(metadata_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(chunks_data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"✓ Metadata saved to: {metadata_file}\")\n",
    "\n",
    "    # Save configuration/statistics\n",
    "    stats_file = os.path.join(output_dir, \"embedding_stats.json\")\n",
    "    stats = {\n",
    "        'model_name': embeddings_info['model_name'],\n",
    "        'embedding_dimensions': embeddings_info['embedding_dim'],\n",
    "        'total_chunks': embeddings_info['num_chunks'],\n",
    "        'batch_size': embeddings_info['batch_size'],\n",
    "        'generation_time_seconds': embeddings_info['generation_time'],\n",
    "        'normalised': embeddings_info['normalised'],\n",
    "        'file_size_bytes': embeddings_info['embeddings'].nbytes,\n",
    "        'file_size_mb': embeddings_info['embeddings'].nbytes / (1024 * 1024),\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "    with open(stats_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(stats, f, indent=2)\n",
    "    print(f\"✓ Statistics saved to: {stats_file}\")\n",
    "    \n",
    "    return {\n",
    "        'embeddings_file': embeddings_file,\n",
    "        'metadata_file': metadata_file,\n",
    "        'stats_file': stats_file,\n",
    "        'stats': stats\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e083b68c",
   "metadata": {},
   "source": [
    "## Display summary\n",
    "- To display infromation about the embedding and information on the model we are using\n",
    "- Verifies if the vectors were normalised or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79af71f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_summary(embeddings_info, save_info):\n",
    "    \"\"\"\n",
    "    Docstring for display_summary which displays comprehensive summary of embeding generation\n",
    "    \n",
    "    :param embeddings_info: list of embeddings and their metadata\n",
    "    :param save_info: where\n",
    "    \"\"\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EMBEDDING GENERATION SUMMARY\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    stats = save_info['stats']\n",
    "    \n",
    "    print(f\"Model Information:\")\n",
    "    print(f\"  Model: {stats['model_name']}\")\n",
    "    print(f\"  Embedding dimensions: {stats['embedding_dimensions']}\")\n",
    "    print(f\"  Normalized: {stats['normalised']}\")\n",
    "    print()\n",
    "\n",
    "    print(f\"Processing Statistics:\")\n",
    "    print(f\"  Total chunks processed: {stats['total_chunks']}\")\n",
    "    print(f\"  Batch size: {stats['batch_size']}\")\n",
    "    print(f\"  Generation time: {stats['generation_time_seconds']:.2f} seconds\")\n",
    "    print(f\"  Throughput: {stats['total_chunks']/stats['generation_time_seconds']:.1f} chunks/second\")\n",
    "    print()\n",
    "\n",
    "    # Validation\n",
    "    print(f\"Validation:\")\n",
    "    if stats['total_chunks'] >= 200:\n",
    "        print(f\"  ✅ PASSED: {stats['total_chunks']} chunks (requirement: 200+)\")\n",
    "    else:\n",
    "        print(f\"  ❌ FAILED: {stats['total_chunks']} chunks (requirement: 200+)\")\n",
    "    \n",
    "    if stats['normalised']:\n",
    "        print(f\"  ✅ PASSED: Vectors normalized for cosine similarity\")\n",
    "    else:\n",
    "        print(f\"  ⚠️  WARNING: Vectors not normalized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ba168c",
   "metadata": {},
   "source": [
    "## Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0dd33379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PART 3: EMBEDDING GENERATION\n",
      "======================================================================\n",
      "    Start time: 2026-01-13 06:16:52\n",
      "Loading chunks from data/processed/all_chunks.json....\n",
      "Loaded 251 chunks\n",
      "    Sample chuck id: educational_chunk_000\n",
      "    Sample category: Educational\n",
      "    Sample text preview: Study of algorithms that improve automatically through experience For the journal, see Machine Learn\n",
      "\n",
      "Step 1: Loaded model 'all-mpnet-base-v2'....\n",
      "    ✓ Model loaded successfully\n",
      "    Embedding dimensions: 768\n",
      "\n",
      "Step 2: Extracting text from 251 chunks.....\n",
      "    ✓ Text extracted\n",
      "\n",
      "Step 3: Generating embeddings (batch_size=32 chunks....)\n",
      "  This will process 251 chunks in 8 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 8/8 [04:34<00:00, 34.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Embeddings generated in 274.48 seconds\n",
      "    Average: 1.094 seconds per chunk\n",
      "    Throughput: 0.9 chunks/second\n",
      "    Normalised: True (all vectors have length ≈ 1.0))\n",
      "\n",
      "======================================================================\n",
      "SAVING EMBEDDINGS\n",
      "======================================================================\n",
      "\n",
      "    ✓ Created directory: data/embeddings\n",
      "    ✓ Embeddings saved to: data/embeddings\\embeddings.npz\n",
      "✓ Metadata saved to: data/embeddings\\chunks_metadata.json\n",
      "✓ Statistics saved to: data/embeddings\\embedding_stats.json\n",
      "\n",
      "======================================================================\n",
      "EMBEDDING GENERATION SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Model Information:\n",
      "  Model: all-mpnet-base-v2\n",
      "  Embedding dimensions: 768\n",
      "  Normalized: True\n",
      "\n",
      "Processing Statistics:\n",
      "  Total chunks processed: 251\n",
      "  Batch size: 32\n",
      "  Generation time: 274.48 seconds\n",
      "  Throughput: 0.9 chunks/second\n",
      "\n",
      "Validation:\n",
      "  ✅ PASSED: 251 chunks (requirement: 200+)\n",
      "  ✅ PASSED: Vectors normalized for cosine similarity\n",
      "\n",
      "End time: 2026-01-13 06:21:34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"PART 3: EMBEDDING GENERATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"    Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "try:\n",
    "    # Step 1: Load chunks from Part 2\n",
    "    chunks_data = load_chunks(\"data/processed/all_chunks.json\")\n",
    "\n",
    "    #Step 2: Generating embeddings\n",
    "    embeddings_info = generateEmbedding(\n",
    "            chunks_data,\n",
    "            model_name='all-mpnet-base-v2',\n",
    "            batch_size=32\n",
    "        )\n",
    "\n",
    "    #Step 3: Save embeddings\n",
    "    save_info = save_embeddings(chunks_data, embeddings_info)\n",
    "\n",
    "    display_summary(embeddings_info, save_info)\n",
    "\n",
    "    print()\n",
    "    print(f\"End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\n❌ ERROR: {e}\")\n",
    "    print(\"Please ensure Part 2 has been completed.\")\n",
    "    raise\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ UNEXPECTED ERROR: {type(e).__name__}: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454db1d3",
   "metadata": {},
   "source": [
    "# Part 4: Vector Database Implementation\n",
    "\n",
    "## Overview\n",
    "This section stores the embeddings generated in Part 3 into ChromaDB, a vector database optimized for similarity search.\n",
    "\n",
    "## Assignment Requirements ✔\n",
    "- ✔ Choose vector database: ChromaDB\n",
    "- ✔ Configure distance metric: Cosine similarity\n",
    "- ✔ Store embeddings with metadata\n",
    "- ✔ Implement efficient indexing: HNSW (automatic)\n",
    "- ✔ Verify data persistence\n",
    "\n",
    "## Configuration\n",
    "- **Database**: ChromaDB (PersistentClient)\n",
    "- **Distance Metric**: Cosine similarity\n",
    "- **Indexing**: HNSW (automatic)\n",
    "- **Batch Size**: 100 chunks per batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dee1210",
   "metadata": {},
   "source": [
    "## Function: Load Data\n",
    "\n",
    "Loads processed chunks from Part 2 and embeddings from Part 3.\n",
    "\n",
    "**Purpose**: Retrieve all data needed for database population\n",
    "\n",
    "**Returns**: Tuple of (chunks_data, embeddings)\n",
    "\n",
    "**Validation**: Verifies that number of chunks matches number of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f4be294",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data() -> Tuple[List[Dict], np.ndarray]:\n",
    "    \"\"\"\n",
    "    Load chunks and embeddings from previous parts.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (chunks_data, embeddings)\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If required data files don't exist\n",
    "        ValueError: If data is corrupted or invalid\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load chunks from Part 2\n",
    "        chunks_file = \"data/processed/all_chunks.json\"\n",
    "        if not os.path.exists(chunks_file):\n",
    "            raise FileNotFoundError(\n",
    "                f\"Chunks file not found: {chunks_file}\\n\"\n",
    "                \"Please run Part 2 first!\"\n",
    "            )\n",
    "        \n",
    "        with open(chunks_file, 'r', encoding='utf-8') as f:\n",
    "            chunks_data = json.load(f)\n",
    "        \n",
    "        if not chunks_data:\n",
    "            raise ValueError(\"Chunks file is empty!\")\n",
    "        \n",
    "        # Load embeddings from Part 3\n",
    "        embeddings_file = \"data/embeddings/embeddings.npz\"\n",
    "        if not os.path.exists(embeddings_file):\n",
    "            raise FileNotFoundError(\n",
    "                f\"Embeddings file not found: {embeddings_file}\\n\"\n",
    "                \"Please run Part 3 first!\"\n",
    "            )\n",
    "        \n",
    "        embeddings_data = np.load(embeddings_file)\n",
    "        embeddings = embeddings_data['embeddings']\n",
    "        \n",
    "        # Verify data consistency\n",
    "        if len(chunks_data) != len(embeddings):\n",
    "            raise ValueError(\n",
    "                f\"Data mismatch: {len(chunks_data)} chunks but \"\n",
    "                f\"{len(embeddings)} embeddings!\"\n",
    "            )\n",
    "        \n",
    "        return chunks_data, embeddings\n",
    "    \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\n❌ ERROR: {e}\")\n",
    "        raise\n",
    "    except ValueError as e:\n",
    "        print(f\"\\n❌ ERROR: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ UNEXPECTED ERROR: {type(e).__name__}: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193978af",
   "metadata": {},
   "source": [
    "## Function: Create ChromaDB Collection\n",
    "\n",
    "Creates and populates the vector database with embeddings and metadata.\n",
    "\n",
    "**Process**:\n",
    "1. Initialize PersistentClient (data saved to disk automatically)\n",
    "2. Create collection with cosine similarity configuration\n",
    "3. Check if collection already exists (avoid duplicates)\n",
    "4. Add data in batches of 100 (ChromaDB has operation limits)\n",
    "\n",
    "**Why batching?** ChromaDB limits single operations to prevent memory issues. We process 100 chunks at a time.\n",
    "\n",
    "**Configuration**: Sets `\"hnsw:space\": \"cosine\"` which enables both cosine similarity and HNSW indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce9756ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chromadb(chunks_data: List[Dict], embeddings: np.ndarray) -> chromadb.Collection:\n",
    "    \"\"\"\n",
    "    Create and populate ChromaDB collection with batching.\n",
    "    \n",
    "    Args:\n",
    "        chunks_data: List of chunk dictionaries from Part 2\n",
    "        embeddings: NumPy array of embeddings from Part 3\n",
    "        \n",
    "    Returns:\n",
    "        chromadb.Collection: Populated collection\n",
    "        \n",
    "    Notes:\n",
    "        - Uses batching to handle ChromaDB limits\n",
    "        - Configures cosine similarity for distance metric\n",
    "        - Data persists to disk automatically\n",
    "    \"\"\"\n",
    "    # Initialize client with persistent storage\n",
    "    client = chromadb.PersistentClient(path=\"data/chromadb\")\n",
    "    \n",
    "    # Create or get collection with cosine similarity\n",
    "    collection = client.get_or_create_collection(\n",
    "        name=\"intelligent_content_retrieval\",\n",
    "        metadata={\n",
    "            \"hnsw:space\": \"cosine\",  # Distance metric\n",
    "            \"description\": \"Multi-domain content for semantic search\",\n",
    "            \"model\": \"all-mpnet-base-v2\",\n",
    "            \"dimensions\": \"768\",\n",
    "            \"created_date\": datetime.now().isoformat()\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Check if collection already has data\n",
    "    existing_count = collection.count()\n",
    "    if existing_count > 0:\n",
    "        print(f\"    ⚠️  Collection already contains {existing_count} documents\")\n",
    "        user_input = input(\"    Delete and rebuild? (yes/no): \").strip().lower()\n",
    "        \n",
    "        if user_input == 'yes':\n",
    "            client.delete_collection(\"intelligent_content_retrieval\")\n",
    "            collection = client.create_collection(\n",
    "                name=\"intelligent_content_retrieval\",\n",
    "                metadata={\n",
    "                    \"hnsw:space\": \"cosine\",\n",
    "                    \"description\": \"Multi-domain content for semantic search\",\n",
    "                    \"model\": \"all-mpnet-base-v2\",\n",
    "                    \"dimensions\": \"768\",\n",
    "                    \"created_date\": datetime.now().isoformat()\n",
    "                }\n",
    "            )\n",
    "            print(\"    ✓ Collection deleted and recreated\")\n",
    "        else:\n",
    "            print(\"    ✓ Using existing collection\")\n",
    "            return collection\n",
    "    \n",
    "    # Add data in batches (ChromaDB has limits on single operations)\n",
    "    batch_size = 100\n",
    "    total_chunks = len(chunks_data)\n",
    "    \n",
    "    print(f\"    Adding {total_chunks} chunks in batches of {batch_size}...\")\n",
    "    \n",
    "    for i in range(0, total_chunks, batch_size):\n",
    "        batch_end = min(i + batch_size, total_chunks)\n",
    "        \n",
    "        # Prepare batch data\n",
    "        batch_ids = [chunk['chunk_id'] for chunk in chunks_data[i:batch_end]]\n",
    "        batch_embeddings = embeddings[i:batch_end].tolist()\n",
    "        batch_documents = [chunk['text'] for chunk in chunks_data[i:batch_end]]\n",
    "        batch_metadatas = [chunk['metadata'] for chunk in chunks_data[i:batch_end]]\n",
    "        \n",
    "        # Add to collection\n",
    "        collection.add(\n",
    "            ids=batch_ids,\n",
    "            embeddings=batch_embeddings,\n",
    "            documents=batch_documents,\n",
    "            metadatas=batch_metadatas\n",
    "        )\n",
    "        \n",
    "        print(f\"    ✓ Batch {i//batch_size + 1}/{(total_chunks + batch_size - 1)//batch_size}: \"\n",
    "              f\"Added chunks {i} to {batch_end-1}\")\n",
    "    \n",
    "    print(f\"    ✓ Successfully added {total_chunks} chunks to ChromaDB!\")\n",
    "    \n",
    "    return collection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f36b188",
   "metadata": {},
   "source": [
    "## Function: Verify Persistence\n",
    "\n",
    "Verifies that data persists across Python sessions by reloading the collection from disk.\n",
    "\n",
    "**Purpose**: Prove that ChromaDB's PersistentClient saves data automatically\n",
    "\n",
    "**How it works**: Creates a new client and loads the existing collection - simulating a Python restart.\n",
    "\n",
    "**Storage**: Data saved to `data/chromadb/` in optimized binary format with HNSW index intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0c39ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_persistence() -> chromadb.Collection:\n",
    "    \"\"\"\n",
    "    Verify that data persists across sessions.\n",
    "    \n",
    "    Returns:\n",
    "        chromadb.Collection: Existing collection loaded from disk\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If collection doesn't exist\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = chromadb.PersistentClient(path=\"data/chromadb\")\n",
    "        collection = client.get_collection(\"intelligent_content_retrieval\")\n",
    "        \n",
    "        count = collection.count()\n",
    "        print(f\"    ✓ Collection loaded from disk\")\n",
    "        print(f\"    ✓ Contains {count} documents\")\n",
    "        print(f\"    ✓ Storage location: data/chromadb/\")\n",
    "        \n",
    "        return collection\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise ValueError(\n",
    "            f\"Collection not found or corrupted: {e}\\n\"\n",
    "            \"You may need to recreate the database.\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe3b466",
   "metadata": {},
   "source": [
    "## Function: Display Database Statistics\n",
    "\n",
    "Displays comprehensive information about the database for documentation and verification.\n",
    "\n",
    "**Shows**:\n",
    "- Collection configuration (name, size, distance metric, model)\n",
    "- Sample documents (first 5 chunks with previews)\n",
    "- Storage information (location, persistence, indexing)\n",
    "\n",
    "**Purpose**: Verify correct configuration and provide data for assignment report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45bc6cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_database_stats(collection: chromadb.Collection) -> None:\n",
    "    \"\"\"\n",
    "    Display comprehensive database statistics.\n",
    "    \n",
    "    Args:\n",
    "        collection: ChromaDB collection to analyze\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"DATABASE STATISTICS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Get basic stats\n",
    "    count = collection.count()\n",
    "    metadata = collection.metadata\n",
    "    \n",
    "    # Get sample data\n",
    "    sample = collection.peek(limit=5)\n",
    "    \n",
    "    print(f\"\\nCollection Information:\")\n",
    "    print(f\"  Name: {collection.name}\")\n",
    "    print(f\"  Total documents: {count}\")\n",
    "    print(f\"  Distance metric: {metadata.get('hnsw:space', 'unknown')}\")\n",
    "    print(f\"  Embedding model: {metadata.get('model', 'unknown')}\")\n",
    "    print(f\"  Embedding dimensions: {metadata.get('dimensions', 'unknown')}\")\n",
    "    \n",
    "    if 'created_date' in metadata:\n",
    "        print(f\"  Created: {metadata['created_date']}\")\n",
    "    \n",
    "    # Display sample documents\n",
    "    print(f\"\\nSample Documents (first 5):\")\n",
    "    for i, (doc_id, doc_text, doc_meta) in enumerate(zip(\n",
    "        sample['ids'],\n",
    "        sample['documents'],\n",
    "        sample['metadatas']\n",
    "    ), 1):\n",
    "        print(f\"  {i}. ID: {doc_id}\")\n",
    "        print(f\"     Category: {doc_meta.get('source_category', 'N/A')}\")\n",
    "        print(f\"     Preview: {doc_text[:80]}...\")\n",
    "    \n",
    "    # Storage information\n",
    "    print(f\"\\nStorage Information:\")\n",
    "    print(f\"  Database path: data/chromadb/\")\n",
    "    print(f\"  Persistence: ✓ Enabled (PersistentClient)\")\n",
    "    print(f\"  Indexing: ✓ HNSW (automatic, efficient nearest neighbor)\")\n",
    "    \n",
    "    print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd9512f",
   "metadata": {},
   "source": [
    "## Function: Save Statistics\n",
    "\n",
    "Saves database statistics to a JSON file for documentation purposes.\n",
    "\n",
    "**Output**: `data/vector_db/database_stats.json`\n",
    "\n",
    "**Contents**: Collection name, document count, metadata, and timestamp\n",
    "\n",
    "**Use**: Include in assignment report and documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ace46dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_database_statistics(collection: chromadb.Collection) -> None:\n",
    "    \"\"\"\n",
    "    Save database statistics to file for documentation.\n",
    "    \n",
    "    Args:\n",
    "        collection: ChromaDB collection to analyze\n",
    "    \"\"\"\n",
    "    stats = {\n",
    "        \"collection_name\": collection.name,\n",
    "        \"total_documents\": collection.count(),\n",
    "        \"metadata\": collection.metadata,\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(\"data/vector_db\", exist_ok=True)\n",
    "    \n",
    "    # Save statistics\n",
    "    stats_file = \"data/vector_db/database_stats.json\"\n",
    "    with open(stats_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(stats, f, indent=2)\n",
    "    \n",
    "    print(f\"    ✓ Statistics saved to: {stats_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f8e4c4",
   "metadata": {},
   "source": [
    "## Main Execution\n",
    "\n",
    "Executes all functions in sequence to complete Part 4.\n",
    "\n",
    "**Steps**:\n",
    "1. Load chunks and embeddings from Parts 2 & 3\n",
    "2. Create ChromaDB collection with batching\n",
    "3. Verify persistence by reloading from disk\n",
    "4. Display database statistics\n",
    "5. Save statistics to JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "abafba8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PART 4: VECTOR DATABASE IMPLEMENTATION\n",
      "======================================================================\n",
      "Start time: 2026-01-13 14:11:14\n",
      "\n",
      "Step 1: Loading data from previous parts...\n",
      "    ✓ Loaded 251 chunks\n",
      "    ✓ Loaded 251 embeddings (768 dimensions)\n",
      "\n",
      "Step 2: Creating ChromaDB collection...\n",
      "    Adding 251 chunks in batches of 100...\n",
      "    ✓ Batch 1/3: Added chunks 0 to 99\n",
      "    ✓ Batch 2/3: Added chunks 100 to 199\n",
      "    ✓ Batch 3/3: Added chunks 200 to 250\n",
      "    ✓ Successfully added 251 chunks to ChromaDB!\n",
      "\n",
      "Step 3: Verifying data persistence...\n",
      "    ✓ Collection loaded from disk\n",
      "    ✓ Contains 251 documents\n",
      "    ✓ Storage location: data/chromadb/\n",
      "\n",
      "Step 4: Analyzing database...\n",
      "\n",
      "======================================================================\n",
      "DATABASE STATISTICS\n",
      "======================================================================\n",
      "\n",
      "Collection Information:\n",
      "  Name: intelligent_content_retrieval\n",
      "  Total documents: 251\n",
      "  Distance metric: cosine\n",
      "  Embedding model: all-mpnet-base-v2\n",
      "  Embedding dimensions: 768\n",
      "  Created: 2026-01-13T14:11:15.276748\n",
      "\n",
      "Sample Documents (first 5):\n",
      "  1. ID: educational_chunk_000\n",
      "     Category: Educational\n",
      "     Preview: Study of algorithms that improve automatically through experience For the journa...\n",
      "  2. ID: educational_chunk_001\n",
      "     Category: Educational\n",
      "     Preview: induction Ontology learning Multimodal learning Supervised learning ( classifica...\n",
      "  3. ID: educational_chunk_002\n",
      "     Category: Educational\n",
      "     Preview: SOM Convolutional neural network U-Net LeNet AlexNet DeepDream Neural field Neur...\n",
      "  4. ID: educational_chunk_003\n",
      "     Category: Educational\n",
      "     Preview: List of datasets for machine-learning research List of datasets in computer visi...\n",
      "  5. ID: educational_chunk_004\n",
      "     Category: Educational\n",
      "     Preview: risk Turing test Uncanny valley Human–AI interaction History Timeline Progress A...\n",
      "\n",
      "Storage Information:\n",
      "  Database path: data/chromadb/\n",
      "  Persistence: ✓ Enabled (PersistentClient)\n",
      "  Indexing: ✓ HNSW (automatic, efficient nearest neighbor)\n",
      "======================================================================\n",
      "\n",
      "Step 5: Saving statistics...\n",
      "    ✓ Statistics saved to: data/vector_db/database_stats.json\n",
      "\n",
      "======================================================================\n",
      "✅ PART 4 COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "Summary:\n",
      "  • Database: ChromaDB (Persistent)\n",
      "  • Collection: intelligent_content_retrieval\n",
      "  • Documents: 251\n",
      "  • Distance metric: Cosine similarity\n",
      "  • Indexing: HNSW (automatic)\n",
      "\n",
      "Next Step: Run Part 5 for semantic search interface\n",
      "End time: 2026-01-13 14:11:16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"PART 4: VECTOR DATABASE IMPLEMENTATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print()\n",
    "\n",
    "try:\n",
    "    # STEP 1: Load data from previous parts\n",
    "    print(\"Step 1: Loading data from previous parts...\")\n",
    "    chunks_data, embeddings = load_data()\n",
    "    print(f\"    ✓ Loaded {len(chunks_data)} chunks\")\n",
    "    print(f\"    ✓ Loaded {len(embeddings)} embeddings ({embeddings.shape[1]} dimensions)\")\n",
    "    print()\n",
    "    \n",
    "    # STEP 2: Create ChromaDB collection\n",
    "    print(\"Step 2: Creating ChromaDB collection...\")\n",
    "    collection = create_chromadb(chunks_data, embeddings)\n",
    "    print()\n",
    "    \n",
    "    # STEP 3: Verify persistence\n",
    "    print(\"Step 3: Verifying data persistence...\")\n",
    "    collection = verify_persistence()\n",
    "    print()\n",
    "    \n",
    "    # STEP 4: Display database statistics\n",
    "    print(\"Step 4: Analyzing database...\")\n",
    "    display_database_stats(collection)\n",
    "    \n",
    "    # STEP 5: Save statistics\n",
    "    print(\"Step 5: Saving statistics...\")\n",
    "    save_database_statistics(collection)\n",
    "    print()\n",
    "    \n",
    "    # Completion message\n",
    "    print(\"=\" * 70)\n",
    "    print(\"✅ PART 4 COMPLETE!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\nSummary:\")\n",
    "    print(f\"  • Database: ChromaDB (Persistent)\")\n",
    "    print(f\"  • Collection: intelligent_content_retrieval\")\n",
    "    print(f\"  • Documents: {collection.count()}\")\n",
    "    print(f\"  • Distance metric: Cosine similarity\")\n",
    "    print(f\"  • Indexing: HNSW (automatic)\")\n",
    "    print(f\"\\nNext Step: Run Part 5 for semantic search interface\")\n",
    "    print(f\"End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\n❌ ERROR: {e}\")\n",
    "    print(\"Please ensure Parts 2 and 3 have been completed.\")\n",
    "    \n",
    "except ValueError as e:\n",
    "    print(f\"\\n❌ ERROR: {e}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ UNEXPECTED ERROR: {type(e).__name__}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a251ad",
   "metadata": {},
   "source": [
    "# Part 5: Semantic Search Interface\n",
    "\n",
    "## Overview\n",
    "This section implements a natural language search interface for querying the vector database created in Part 4.\n",
    "\n",
    "## Assignment Requirements ✔\n",
    "- ✔ Accept natural language queries\n",
    "- ✔ Return top-k most relevant results (k=5 minimum)\n",
    "- ✔ Display relevance scores\n",
    "- ✔ Show source metadata for each result\n",
    "- ✔ Test with at least 5 diverse queries\n",
    "\n",
    "## Search Modes\n",
    "1. **Semantic Search**: Pure AI-powered similarity matching using embeddings\n",
    "2. **Hybrid Search**: Combines semantic similarity with keyword matching (BONUS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d783c4",
   "metadata": {},
   "source": [
    "## Load Database & Model\n",
    "\n",
    "Load the ChromaDB collection created in Part 4 and the same embedding model used in Part 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "880310ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Database loaded: 251 chunks\n",
      "✓ Model loaded: 768 dimensions\n"
     ]
    }
   ],
   "source": [
    "# Load the ChromaDB collection from Part 4\n",
    "client = chromadb.PersistentClient(path=\"data/chromadb\")\n",
    "collection = client.get_collection(name=\"intelligent_content_retrieval\")\n",
    "\n",
    "print(f\"✓ Database loaded: {collection.count()} chunks\")\n",
    "\n",
    "# Load the same embedding model used in Part 3\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "print(f\"✓ Model loaded: {model.get_sentence_embedding_dimension()} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7297abae",
   "metadata": {},
   "source": [
    "## Semantic Search Theory\n",
    "\n",
    "### How Semantic Search Works\n",
    "\n",
    "1. **Query Encoding**: Convert query → 768D embedding\n",
    "2. **Similarity Calculation**: Compare with stored embeddings using cosine similarity\n",
    "3. **Ranking**: Sort by similarity score\n",
    "4. **Return Top-K**: Return K most similar chunks\n",
    "\n",
    "### Similarity Score\n",
    "\n",
    "**Formula**: `similarity = 1 - distance`\n",
    "\n",
    "**Range**: 0 (different) to 1 (identical)\n",
    "- **HIGH** (> 0.7): Very similar\n",
    "- **MEDIUM** (> 0.5): Related\n",
    "- **LOW** (≤ 0.5): Tangential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb4cbb9",
   "metadata": {},
   "source": [
    "## Function: Semantic Search\n",
    "\n",
    "Performs semantic search by encoding the query and finding similar chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "453fca8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(\n",
    "    collection: chromadb.Collection,\n",
    "    model: SentenceTransformer,\n",
    "    query_text: str,\n",
    "    n_results: int = 5,\n",
    "    filter_category: str = None\n",
    ") -> Dict:\n",
    "    \"\"\"Perform semantic search on the existing database.\"\"\"\n",
    "    print(f\"\\n🔍 Searching: '{query_text}'\")\n",
    "    if filter_category:\n",
    "        print(f\"📁 Filter: {filter_category}\")\n",
    "    print(\"⚙️  Mode: Semantic Search\")\n",
    "    \n",
    "    query_embedding = model.encode(\n",
    "        query_text,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True\n",
    "    )\n",
    "    \n",
    "    query_params = {\n",
    "        \"query_embeddings\": [query_embedding.tolist()],\n",
    "        \"n_results\": n_results\n",
    "    }\n",
    "    \n",
    "    if filter_category:\n",
    "        query_params[\"where\"] = {\"source_category\": filter_category}\n",
    "    \n",
    "    results = collection.query(**query_params)\n",
    "    \n",
    "    print(f\"✓ Found {len(results['documents'][0])} results\\n\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a0f04f",
   "metadata": {},
   "source": [
    "## Function: Calculate Keyword Score\n",
    "\n",
    "Calculates the proportion of keywords found in a text chunk (for hybrid search)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d076b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_keyword_score(text: str, keywords: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate keyword match score (proportion of keywords found).\n",
    "    \n",
    "    Returns value between 0.0 and 1.0\n",
    "    \"\"\"\n",
    "    if not keywords or len(keywords) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    matches = 0\n",
    "    \n",
    "    for keyword in keywords:\n",
    "        keyword_lower = keyword.lower().strip()\n",
    "        if keyword_lower in text_lower:\n",
    "            matches += 1\n",
    "    \n",
    "    return matches / len(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b63f163",
   "metadata": {},
   "source": [
    "## Function: Hybrid Search (BONUS)\n",
    "\n",
    "Combines semantic similarity with keyword matching for improved precision.\n",
    "\n",
    "**Process**:\n",
    "1. Retrieve 10× candidates using semantic search\n",
    "2. Calculate keyword scores\n",
    "3. Compute hybrid score: `(0.7 × semantic) + (0.3 × keyword)`\n",
    "4. Re-rank and return top-K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "013190a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search(\n",
    "    collection: chromadb.Collection,\n",
    "    model: SentenceTransformer,\n",
    "    query_text: str,\n",
    "    keywords: List[str] = None,\n",
    "    n_results: int = 5,\n",
    "    filter_category: str = None,\n",
    "    semantic_weight: float = 0.7,\n",
    "    keyword_weight: float = 0.3\n",
    ") -> List[Dict]:\n",
    "    \"\"\"Perform hybrid search combining semantic similarity with keyword matching.\"\"\"\n",
    "    print(f\"\\n🔍 Searching: '{query_text}'\")\n",
    "    if filter_category:\n",
    "        print(f\"📁 Filter: {filter_category}\")\n",
    "    if keywords:\n",
    "        print(f\"🔑 Keywords: {', '.join(keywords)}\")\n",
    "    print(f\"⚙️  Mode: Hybrid Search ({semantic_weight*100:.0f}% semantic + {keyword_weight*100:.0f}% keyword)\")\n",
    "    \n",
    "    # STEP 1: Get MORE semantic results for re-ranking\n",
    "    retrieval_count = min(n_results * 10, 100)\n",
    "    \n",
    "    query_embedding = model.encode(\n",
    "        query_text,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True\n",
    "    )\n",
    "    \n",
    "    query_params = {\n",
    "        \"query_embeddings\": [query_embedding.tolist()],\n",
    "        \"n_results\": retrieval_count\n",
    "    }\n",
    "    \n",
    "    if filter_category:\n",
    "        query_params[\"where\"] = {\"source_category\": filter_category}\n",
    "    \n",
    "    semantic_results = collection.query(**query_params)\n",
    "    \n",
    "    # If no keywords, return semantic results\n",
    "    if not keywords or len(keywords) == 0:\n",
    "        print(\"⚠️  No keywords - using semantic search only\")\n",
    "        return {\n",
    "            'documents': [semantic_results['documents'][0][:n_results]],\n",
    "            'metadatas': [semantic_results['metadatas'][0][:n_results]],\n",
    "            'distances': [semantic_results['distances'][0][:n_results]]\n",
    "        }\n",
    "    \n",
    "    # STEP 2: Calculate hybrid scores\n",
    "    hybrid_results = []\n",
    "    \n",
    "    for doc, metadata, distance in zip(\n",
    "        semantic_results['documents'][0],\n",
    "        semantic_results['metadatas'][0],\n",
    "        semantic_results['distances'][0]\n",
    "    ):\n",
    "        semantic_score = 1 - distance\n",
    "        keyword_score = calculate_keyword_score(doc, keywords)\n",
    "        \n",
    "        hybrid_score = (semantic_weight * semantic_score) + (keyword_weight * keyword_score)\n",
    "        \n",
    "        hybrid_results.append({\n",
    "            'document': doc,\n",
    "            'metadata': metadata,\n",
    "            'semantic_score': semantic_score,\n",
    "            'keyword_score': keyword_score,\n",
    "            'hybrid_score': hybrid_score,\n",
    "            'distance': distance\n",
    "        })\n",
    "    \n",
    "    # STEP 3: Sort by hybrid score (higher = better)\n",
    "    hybrid_results.sort(key=lambda x: x['hybrid_score'], reverse=True)\n",
    "    \n",
    "    # STEP 4: Return top N results\n",
    "    top_results = hybrid_results[:n_results]\n",
    "    \n",
    "    print(f\"✓ Found {len(top_results)} results (from {retrieval_count} candidates)\\n\")\n",
    "    \n",
    "    return top_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c65470a",
   "metadata": {},
   "source": [
    "## Function: Display Results\n",
    "\n",
    "Displays search results with relevance scores and metadata. Handles both semantic and hybrid result formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "77276861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(results, query_text: str, search_mode: str = \"semantic\") -> None:\n",
    "    \"\"\"Display search results in a CLEAN, readable format.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"{'HYBRID' if search_mode == 'hybrid' else 'SEMANTIC'} SEARCH RESULTS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Query: {query_text}\\n\")\n",
    "    \n",
    "    # Handle both result formats\n",
    "    if search_mode == \"semantic\":\n",
    "        if not results['documents'][0]:\n",
    "            print(\"❌ No results found!\")\n",
    "            print(\"💡 Try rephrasing your query or removing filters.\\n\")\n",
    "            return\n",
    "        \n",
    "        for i, (doc, metadata, distance) in enumerate(zip(\n",
    "            results['documents'][0],\n",
    "            results['metadatas'][0],\n",
    "            results['distances'][0]\n",
    "        ), 1):\n",
    "            similarity = 1 - distance\n",
    "            \n",
    "            # Relevance indicator\n",
    "            if similarity > 0.7:\n",
    "                indicator = \"🔥 HIGH\"\n",
    "            elif similarity > 0.5:\n",
    "                indicator = \"✓ MEDIUM\"\n",
    "            else:\n",
    "                indicator = \"⚠️  LOW\"\n",
    "            \n",
    "            print(f\"\\n📄 Result #{i}  |  Similarity: {similarity:.3f} {indicator}\")\n",
    "            print(f\"   {metadata['source_category']} | Chunk {metadata.get('chunk_index', '?')}/{metadata.get('total_chunks_from_source', '?')}\")\n",
    "            print(f\"   {metadata['source_url'][:65]}...\")\n",
    "            print(f\"\\n   {doc[:250]}...\\n\")\n",
    "    \n",
    "    else:\n",
    "        # Hybrid results\n",
    "        if not results:\n",
    "            print(\"❌ No results found!\")\n",
    "            print(\"💡 Try different keywords or rephrasing.\\n\")\n",
    "            return\n",
    "        \n",
    "        for i, result in enumerate(results, 1):\n",
    "            hybrid = result['hybrid_score']\n",
    "            \n",
    "            if hybrid > 0.7:\n",
    "                indicator = \"🔥 HIGH\"\n",
    "            elif hybrid > 0.5:\n",
    "                indicator = \"✓ MEDIUM\"\n",
    "            else:\n",
    "                indicator = \"⚠️  LOW\"\n",
    "            \n",
    "            print(f\"\\n📄 Result #{i}  |  Hybrid: {hybrid:.3f} {indicator}\")\n",
    "            print(f\"   Semantic: {result['semantic_score']:.3f} | Keywords: {result['keyword_score']:.3f}\")\n",
    "            print(f\"   {result['metadata']['source_category']} | Chunk {result['metadata'].get('chunk_index', '?')}/{result['metadata'].get('total_chunks_from_source', '?')}\")\n",
    "            print(f\"   {result['metadata']['source_url'][:65]}...\")\n",
    "            print(f\"\\n   {result['document'][:250]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14357f7",
   "metadata": {},
   "source": [
    "## Test Queries (Assignment Requirement)\n",
    "\n",
    "Testing with **5 diverse queries** as required. Each query demonstrates different search capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3493671",
   "metadata": {},
   "source": [
    "### Test Query 1: Definition Query\n",
    "\n",
    "Simple definition query to test concept identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "81e17384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Searching: 'What is machine learning?'\n",
      "⚙️  Mode: Semantic Search\n",
      "✓ Found 5 results\n",
      "\n",
      "\n",
      "======================================================================\n",
      "SEMANTIC SEARCH RESULTS\n",
      "======================================================================\n",
      "Query: What is machine learning?\n",
      "\n",
      "\n",
      "📄 Result #1  |  Similarity: 0.634 ✓ MEDIUM\n",
      "   Educational | Chunk 5/128\n",
      "   https://en.wikipedia.org/wiki/Machine_learning...\n",
      "\n",
      "   , speech recognition , email filtering , agriculture , and medicine . The application of ML to business problems is known as predictive analytics . Statistics and mathematical optimisation (mathematical programming) methods comprise the foundations o...\n",
      "\n",
      "\n",
      "📄 Result #2  |  Similarity: 0.630 ✓ MEDIUM\n",
      "   Educational | Chunk 53/128\n",
      "   https://en.wikipedia.org/wiki/Machine_learning...\n",
      "\n",
      "   includes learning classifier systems , [ 98 ] association rule learning , [ 99 ] artificial immune systems , [ 100 ] and other similar models. These methods extract patterns from data and evolve rules over time. Training models [ edit ] Typically, ma...\n",
      "\n",
      "\n",
      "📄 Result #3  |  Similarity: 0.628 ✓ MEDIUM\n",
      "   Educational | Chunk 22/128\n",
      "   https://en.wikipedia.org/wiki/Machine_learning...\n",
      "\n",
      "   interacts with a dynamic environment in which it must perform a certain goal (such as driving a vehicle or playing a game against an opponent). As it navigates its problem space, the program is provided feedback that's analogous to rewards, which it ...\n",
      "\n",
      "\n",
      "📄 Result #4  |  Similarity: 0.622 ✓ MEDIUM\n",
      "   Educational | Chunk 57/128\n",
      "   https://en.wikipedia.org/wiki/Machine_learning...\n",
      "\n",
      "   Khosla , predicted that 80% of medical doctors jobs would be lost in the next two decades to automated machine learning medical diagnostic software. [ 108 ] In 2014, it was reported that a machine learning algorithm had been applied in the field of a...\n",
      "\n",
      "\n",
      "📄 Result #5  |  Similarity: 0.618 ✓ MEDIUM\n",
      "   Educational | Chunk 36/128\n",
      "   https://en.wikipedia.org/wiki/Machine_learning...\n",
      "\n",
      "   that the majority of the instances in the data set are normal, by looking for instances that seem to fit the least to the remainder of the data set. Supervised anomaly detection techniques require a data set that has been labelled as \"normal\" and \"ab...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query1 = \"What is machine learning?\"\n",
    "results1 = semantic_search(collection, model, query1, n_results=5)\n",
    "display_results(results1, query1, \"semantic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca279b84",
   "metadata": {},
   "source": [
    "### Test Query 2: Conceptual/How-To Query\n",
    "\n",
    "Testing explanation of processes and mechanisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d6aea8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Searching: 'How does the NPT prevent states from acquiring nuclear weapons ?'\n",
      "⚙️  Mode: Semantic Search\n",
      "✓ Found 5 results\n",
      "\n",
      "\n",
      "======================================================================\n",
      "SEMANTIC SEARCH RESULTS\n",
      "======================================================================\n",
      "Query: How does the NPT prevent states from acquiring nuclear weapons ?\n",
      "\n",
      "\n",
      "📄 Result #1  |  Similarity: 0.684 ✓ MEDIUM\n",
      "   Research Publication | Chunk 6/69\n",
      "   https://pmc.ncbi.nlm.nih.gov/articles/PMC4165831/...\n",
      "\n",
      "   enjoyed during the international nuclear disarmament policies, of which the most representative example is the NPT (Grotto 2010 ; Prăvălie 2012 ). These countries are part of the nuclear weapon states category, allowed to own nuclear weapons, as stip...\n",
      "\n",
      "\n",
      "📄 Result #2  |  Similarity: 0.674 ✓ MEDIUM\n",
      "   Research Publication | Chunk 4/69\n",
      "   https://pmc.ncbi.nlm.nih.gov/articles/PMC4165831/...\n",
      "\n",
      "   impact in limiting radioactive isotopes in the atmosphere in the two hemispheres from 1963 on (Levin et al. 1994 ; Manning and Melhuish 1994 ). The entry into force of the Non-Proliferation Treaty (NPT) in 1968, banning nuclear arming of all states o...\n",
      "\n",
      "\n",
      "📄 Result #3  |  Similarity: 0.524 ✓ MEDIUM\n",
      "   Research Publication | Chunk 3/69\n",
      "   https://pmc.ncbi.nlm.nih.gov/articles/PMC4165831/...\n",
      "\n",
      "   (the Castle Bravo test); and in 1961, by the USSR, in the Novaia Zemlia archipelago, north of the Ural mountains (the Tsar test) (Goodby 2005 ). The severe environmental damage caused by these nuclear tests, the most powerful ever to be conducted in ...\n",
      "\n",
      "\n",
      "📄 Result #4  |  Similarity: 0.477 ⚠️  LOW\n",
      "   Research Publication | Chunk 5/69\n",
      "   https://pmc.ncbi.nlm.nih.gov/articles/PMC4165831/...\n",
      "\n",
      "   a new phase in stopping all types of nuclear tests began, with the United Nations adoption of the Comprehensive Nuclear-Test-Ban Treaty (CTBT 1996 ). However, the large number of nuclear weapons tests carried out in the atmosphere and underground dur...\n",
      "\n",
      "\n",
      "📄 Result #5  |  Similarity: 0.473 ⚠️  LOW\n",
      "   Research Publication | Chunk 55/69\n",
      "   https://pmc.ncbi.nlm.nih.gov/articles/PMC4165831/...\n",
      "\n",
      "   cancer rates and 131I doses from Nevada atmospheric nuclear bomb tests: An update. Radiation Research. 2010;173:659–664. doi: 10.1667/RR2057.1. [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ] Goodby J. The Limited Test Ban Negotiations, 19...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query2 = \"How does the NPT prevent states from acquiring nuclear weapons ?\"\n",
    "results2 = semantic_search(collection, model, query2, n_results=5)\n",
    "display_results(results2, query2, \"semantic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54773006",
   "metadata": {},
   "source": [
    "### Test Query 3: Comparison Query\n",
    "\n",
    "Testing ability to find comparative content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d8713541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Searching: 'Explain the difference between the 2 main categories of nuclear armed states '\n",
      "⚙️  Mode: Semantic Search\n",
      "✓ Found 5 results\n",
      "\n",
      "\n",
      "======================================================================\n",
      "SEMANTIC SEARCH RESULTS\n",
      "======================================================================\n",
      "Query: Explain the difference between the 2 main categories of nuclear armed states \n",
      "\n",
      "\n",
      "📄 Result #1  |  Similarity: 0.684 ✓ MEDIUM\n",
      "   Research Publication | Chunk 6/69\n",
      "   https://pmc.ncbi.nlm.nih.gov/articles/PMC4165831/...\n",
      "\n",
      "   enjoyed during the international nuclear disarmament policies, of which the most representative example is the NPT (Grotto 2010 ; Prăvălie 2012 ). These countries are part of the nuclear weapon states category, allowed to own nuclear weapons, as stip...\n",
      "\n",
      "\n",
      "📄 Result #2  |  Similarity: 0.533 ✓ MEDIUM\n",
      "   Research Publication | Chunk 7/69\n",
      "   https://pmc.ncbi.nlm.nih.gov/articles/PMC4165831/...\n",
      "\n",
      "   was one of the main ways of asserting nuclear power status, as well as the place held by these states in the hierarchy of nuclear geopolitics. According to the data provided by the Stockholm International Peace Research Institute platform, 2053 nucle...\n",
      "\n",
      "\n",
      "📄 Result #3  |  Similarity: 0.504 ✓ MEDIUM\n",
      "   Research Publication | Chunk 4/69\n",
      "   https://pmc.ncbi.nlm.nih.gov/articles/PMC4165831/...\n",
      "\n",
      "   impact in limiting radioactive isotopes in the atmosphere in the two hemispheres from 1963 on (Levin et al. 1994 ; Manning and Melhuish 1994 ). The entry into force of the Non-Proliferation Treaty (NPT) in 1968, banning nuclear arming of all states o...\n",
      "\n",
      "\n",
      "📄 Result #4  |  Similarity: 0.461 ⚠️  LOW\n",
      "   Research Publication | Chunk 5/69\n",
      "   https://pmc.ncbi.nlm.nih.gov/articles/PMC4165831/...\n",
      "\n",
      "   a new phase in stopping all types of nuclear tests began, with the United Nations adoption of the Comprehensive Nuclear-Test-Ban Treaty (CTBT 1996 ). However, the large number of nuclear weapons tests carried out in the atmosphere and underground dur...\n",
      "\n",
      "\n",
      "📄 Result #5  |  Similarity: 0.453 ⚠️  LOW\n",
      "   Research Publication | Chunk 55/69\n",
      "   https://pmc.ncbi.nlm.nih.gov/articles/PMC4165831/...\n",
      "\n",
      "   cancer rates and 131I doses from Nevada atmospheric nuclear bomb tests: An update. Radiation Research. 2010;173:659–664. doi: 10.1667/RR2057.1. [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ] Goodby J. The Limited Test Ban Negotiations, 19...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query3 = \"Explain the difference between the 2 main categories of nuclear armed states \"\n",
    "results3 = semantic_search(collection, model, query3, n_results=5)\n",
    "display_results(results3, query3, \"semantic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cad2b26",
   "metadata": {},
   "source": [
    "### Test Query 4: Domain-Specific Query\n",
    "\n",
    "Testing retrieval of application-focused content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4af235c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Searching: 'What are the applications of artificial intelligence in healthcare?'\n",
      "⚙️  Mode: Semantic Search\n",
      "✓ Found 5 results\n",
      "\n",
      "\n",
      "======================================================================\n",
      "SEMANTIC SEARCH RESULTS\n",
      "======================================================================\n",
      "Query: What are the applications of artificial intelligence in healthcare?\n",
      "\n",
      "\n",
      "📄 Result #1  |  Similarity: 0.533 ✓ MEDIUM\n",
      "   Educational | Chunk 70/128\n",
      "   https://en.wikipedia.org/wiki/Machine_learning...\n",
      "\n",
      "   the algorithms could be designed to provide patients with unnecessary tests or medication in which the algorithm's proprietary owners hold stakes. There is potential for machine learning in health care to provide professionals with an additional tool...\n",
      "\n",
      "\n",
      "📄 Result #2  |  Similarity: 0.520 ✓ MEDIUM\n",
      "   Educational | Chunk 60/128\n",
      "   https://en.wikipedia.org/wiki/Machine_learning...\n",
      "\n",
      "   for the decisions\" it makes. [ 127 ] In 2018, a self-driving car from Uber failed to detect a pedestrian, who was killed after a collision. [ 128 ] Attempts to use machine learning in healthcare with the IBM Watson system failed to deliver even after...\n",
      "\n",
      "\n",
      "📄 Result #3  |  Similarity: 0.509 ✓ MEDIUM\n",
      "   Educational | Chunk 69/128\n",
      "   https://en.wikipedia.org/wiki/Machine_learning...\n",
      "\n",
      "   in many other systems. [ 161 ] Because of such challenges, the effective use of machine learning may take longer to be adopted in other domains. [ 162 ] Concern for fairness in machine learning, that is, reducing bias in machine learning and propelli...\n",
      "\n",
      "\n",
      "📄 Result #4  |  Similarity: 0.507 ✓ MEDIUM\n",
      "   Educational | Chunk 10/128\n",
      "   https://en.wikipedia.org/wiki/Machine_learning...\n",
      "\n",
      "   intelligence (AI). In the early days of AI as an academic discipline , some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed \" neura...\n",
      "\n",
      "\n",
      "📄 Result #5  |  Similarity: 0.499 ⚠️  LOW\n",
      "   Educational | Chunk 65/128\n",
      "   https://en.wikipedia.org/wiki/Machine_learning...\n",
      "\n",
      "   . [ edit ] The ethics of artificial intelligence covers a broad range of topics within AI that are considered to have particular ethical stakes. [ 146 ] This includes algorithmic biases , fairness , accountability , transparency, privacy, and regulat...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query4 = \"What are the applications of artificial intelligence in healthcare?\"\n",
    "results4 = semantic_search(collection, model, query4, n_results=5)\n",
    "display_results(results4, query4, \"semantic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52737a1d",
   "metadata": {},
   "source": [
    "### Test Query 5: Procedural Query\n",
    "\n",
    "Testing technical procedure explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "59b7e108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Searching: 'How do I convert a normal Python TensorFlow function into a graph using tf.function?'\n",
      "⚙️  Mode: Semantic Search\n",
      "✓ Found 5 results\n",
      "\n",
      "\n",
      "======================================================================\n",
      "SEMANTIC SEARCH RESULTS\n",
      "======================================================================\n",
      "Query: How do I convert a normal Python TensorFlow function into a graph using tf.function?\n",
      "\n",
      "\n",
      "📄 Result #1  |  Similarity: 0.768 🔥 HIGH\n",
      "   Technical Documentation | Chunk 4/34\n",
      "   https://www.tensorflow.org/guide/intro_to_graphs...\n",
      "\n",
      "   , either as a direct call or as a decorator. tf.function takes a regular function as input and returns a tf.types.experimental.PolymorphicFunction . A PolymorphicFunction is a Python callable that builds TensorFlow graphs from the Python function. Yo...\n",
      "\n",
      "\n",
      "📄 Result #2  |  Similarity: 0.754 🔥 HIGH\n",
      "   Technical Documentation | Chunk 13/34\n",
      "   https://www.tensorflow.org/guide/intro_to_graphs...\n",
      "\n",
      "   to The benefits of graphs above). tf.function applies to a function and all other functions it calls : def inner_function ( x , y , b ): x = tf . matmul ( x , y ) x = x + b return x # Using the `tf.function` decorator makes `outer_function` into a # ...\n",
      "\n",
      "\n",
      "📄 Result #3  |  Similarity: 0.738 🔥 HIGH\n",
      "   Technical Documentation | Chunk 14/34\n",
      "   https://www.tensorflow.org/guide/intro_to_graphs...\n",
      "\n",
      "   TensorFlow operations are easily captured by a tf.Graph , Python-specific logic needs to undergo an extra step in order to become part of the graph. tf.function uses a library called AutoGraph ( tf.autograph ) to convert Python code into graph-genera...\n",
      "\n",
      "\n",
      "📄 Result #4  |  Similarity: 0.726 🔥 HIGH\n",
      "   Technical Documentation | Chunk 23/34\n",
      "   https://www.tensorflow.org/guide/intro_to_graphs...\n",
      "\n",
      "   are three `ConcreteFunction`s (one for each graph) in `my_relu`. # The `ConcreteFunction` also knows the return type and shape! print ( my_relu . pretty_printed_concrete_signatures ()) Input Parameters: x (POSITIONAL_OR_KEYWORD): TensorSpec(shape=(),...\n",
      "\n",
      "\n",
      "📄 Result #5  |  Similarity: 0.705 🔥 HIGH\n",
      "   Technical Documentation | Chunk 28/34\n",
      "   https://www.tensorflow.org/guide/intro_to_graphs...\n",
      "\n",
      "   Do not rely on an error being raised while executing a graph. def unused_return_eager ( x ): # Get index 1 will fail when `len(x) == 1` tf . gather ( x , [ 1 ]) # unused return x try : print ( unused_return_eager ( tf . constant ([ 0.0 ]))) except tf...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query5 = \"How do I convert a normal Python TensorFlow function into a graph using tf.function?\"\n",
    "results5 = semantic_search(collection, model, query5, n_results=5)\n",
    "display_results(results5, query5, \"semantic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d6f1fc",
   "metadata": {},
   "source": [
    "## BONUS: Hybrid Search Example\n",
    "\n",
    "Demonstrating hybrid search with keyword matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "95d82a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Searching: 'machine learning algorithms'\n",
      "🔑 Keywords: machine, learning, algorithms, neural\n",
      "⚙️  Mode: Hybrid Search (70% semantic + 30% keyword)\n",
      "✓ Found 5 results (from 50 candidates)\n",
      "\n",
      "\n",
      "======================================================================\n",
      "HYBRID SEARCH RESULTS\n",
      "======================================================================\n",
      "Query: machine learning algorithms\n",
      "\n",
      "\n",
      "📄 Result #1  |  Hybrid: 0.707 🔥 HIGH\n",
      "   Semantic: 0.581 | Keywords: 1.000\n",
      "   Educational | Chunk 116/128\n",
      "   https://en.wikipedia.org/wiki/Machine_learning...\n",
      "\n",
      "   York: Oxford University Press. ISBN 978-0-19-510270-3 . Archived from the original on 26 July 2020 . Retrieved 22 August 2020 . Russell, Stuart J. ; Norvig, Peter (2003), Artificial Intelligence: A Modern Approach (2nd ed.), Upper Saddle River, New J...\n",
      "\n",
      "\n",
      "📄 Result #2  |  Hybrid: 0.705 🔥 HIGH\n",
      "   Semantic: 0.578 | Keywords: 1.000\n",
      "   Educational | Chunk 79/128\n",
      "   https://en.wikipedia.org/wiki/Machine_learning...\n",
      "\n",
      "   ^ Nilsson, Nils J. (1965). Learning Machines . McGraw-Hill. ^ Duda, R., Hart P. Pattern Recognition and Scene Analysis, Wiley Interscience, 1973 ^ S. Bozinovski, \"Teaching space: A representation concept for adaptive pattern classification\" COINS Tec...\n",
      "\n",
      "\n",
      "📄 Result #3  |  Hybrid: 0.696 ✓ MEDIUM\n",
      "   Semantic: 0.565 | Keywords: 1.000\n",
      "   Educational | Chunk 74/128\n",
      "   https://en.wikipedia.org/wiki/Machine_learning...\n",
      "\n",
      "   of machine learning algorithms include the following: Free and open-source software [ edit ] See also: Lists of open-source artificial intelligence software Caffe Deeplearning4j DeepSpeed ELKI Google JAX Infer.NET JASP Jubatus Keras Kubeflow LightGBM...\n",
      "\n",
      "\n",
      "📄 Result #4  |  Hybrid: 0.684 ✓ MEDIUM\n",
      "   Semantic: 0.549 | Keywords: 1.000\n",
      "   Educational | Chunk 76/128\n",
      "   https://en.wikipedia.org/wiki/Machine_learning...\n",
      "\n",
      "   ( KDD ) Conference on Neural Information Processing Systems ( NeurIPS ) See also [ edit ] Automated machine learning – Process of automating the application of machine learning Big data – Extremely large or complex datasets Deep learning — branch of ...\n",
      "\n",
      "\n",
      "📄 Result #5  |  Hybrid: 0.671 ✓ MEDIUM\n",
      "   Semantic: 0.529 | Keywords: 1.000\n",
      "   Educational | Chunk 8/128\n",
      "   https://en.wikipedia.org/wiki/Machine_learning...\n",
      "\n",
      "   an artificial neural network learns to recognise 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal. [ 14 ] Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine l...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hybrid search example\n",
    "query_hybrid = \"machine learning algorithms\"\n",
    "keywords_hybrid = [\"machine\", \"learning\", \"algorithms\", \"neural\"]\n",
    "\n",
    "results_hybrid = hybrid_search(\n",
    "    collection, \n",
    "    model, \n",
    "    query_hybrid, \n",
    "    keywords_hybrid,\n",
    "    n_results=5,\n",
    "    semantic_weight=0.7,\n",
    "    keyword_weight=0.3\n",
    ")\n",
    "\n",
    "display_results(results_hybrid, query_hybrid, \"hybrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5ff4ee",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Part 5 Accomplishments \n",
    "\n",
    "1.  **Semantic Search**: Natural language query interface\n",
    "2.  **Top-K Results**: Returns top 5 most relevant results\n",
    "3.  **Relevance Scores**: Displays similarity scores with indicators\n",
    "4.  **Source Metadata**: Shows category, URL, chunk info\n",
    "5.  **Diverse Queries**: Tested with 5+ query types\n",
    "\n",
    "### BONUS Features \n",
    "\n",
    "1. **Hybrid Search**: Semantic + keyword matching\n",
    "2. **Configurable Weights**: Adjustable semantic/keyword balance\n",
    "3. **Smart Re-ranking**: 10× candidate retrieval\n",
    "\n",
    "### Additional Implementations\n",
    "\n",
    "For the complete system with interactive CLI, web interface, and API:\n",
    "- `part05SearchInterface.py` - Full CLI version\n",
    "- `app.py` - Flask web application\n",
    "- `llm_enhancer.py` - LLM enhancement via claude api\n",
    "\n",
    "**Web Interface**: https://contentretrievalfrontend.vercel.app/\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Successfully implemented semantic search system with:\n",
    "- AI-powered natural language understanding\n",
    "- Efficient vector database retrieval\n",
    "- Accurate cosine similarity ranking\n",
    "- Bonus hybrid search capability"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
